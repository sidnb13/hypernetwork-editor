defaults:
  - _self_
  - model: gpt2
  - task: wikipedia

mode: train

out_dir: ./assets
ckpt_dir: ${out_dir}/checkpoints
eval_dir: ${out_dir}/eval
data_dir: ${out_dir}/data
resume_ckpt: null

exp_name: null
seed: 42
debug: false

train:
  use_ddp: false
  ckpt_folder: null
  lr: 4e-4
  steps: -1
  warmup_steps: 0.1
  n_epochs: 1
  train_batch_size: 32
  validation_batch_size: 32
  scheduler: constant
  optim: AdamW
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  max_grad_norm: 1.0

  # edit specific hparams
  lambda: 0
  lambda_testing_penalty: 0
  loss: kl # or ce
  stop_editing_idx: 8

  # checkpointing, evaluation, and logging
  log_interval: 100
  eval_interval: 100
  save_interval: 500
  do_save: true
  do_eval: true

data:
  train_split_name: train
  test_split_name: test
  val_split_name: val
  padding_side: right
  test_split: 0.1
  val_split: 0.1
  n_examples: -1
  target_generation_tokens: 128 # 50 for wiki task

eval:
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  max_new_tokens: 128
  metrics: [em, f1]
  visualize_interval: 10

wandb:
  enabled: false
  project: hypernetwork-editor
  entity: ${oc.env:WANDB_ENTITY,michaelsklar}
  tags: []
  group: null
  notes: null
  run_id: null
  resume: false
