{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb\n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_index = 8  # 4090 or A6000\n",
    "num_editing_heads = (\n",
    "    768 * 2\n",
    ")  # more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "editor_channel_width = 768 * 2\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 3e-4\n",
    "edit_dampening_factor = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes from Sid: SAE vs principal components\n",
    "-you could try to work on the SAE's???\n",
    "-get discrete targets\n",
    "[I wouldn't be surprised at all if this helped a lot]\n",
    "\n",
    "-Next deliverables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['tokenized_first_sentence'][30].size(), batch['tokenized_next_50_tokens'][30].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"hypernetworks\",\n",
    "#     config={\"targetmodel\": \"gpt2\", \"editormodel\": \"gpt2\"},\n",
    "# )\n",
    "# # Copy this below where needed!\n",
    "# # run.log_model(path=\"<path-to-model>\", name=\"<name>\")\n",
    "\n",
    "# # wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Set torch default device\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_attention_to_layer(block, config):\n",
    "    block.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(\n",
    "        normalized_shape=768, eps=config.layer_norm_epsilon\n",
    "    )\n",
    "    original_query_weights = block.attn.c_attn.weight[:, :768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:, 768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias = nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Controls whether the head will do a global softmax in all positions & layers\n",
    "        # If True, the attn is global and will sum to 1\n",
    "        # If False, the attn is a logistic fxn independently for every layer & token\n",
    "        # I suspect we will also want to penalize the intervention norm\n",
    "        self.num_editing_heads = (\n",
    "            config.num_editing_heads\n",
    "        )  # should default to 1, but we're going to test adding more\n",
    "        self.edit_channel_width = config.edit_channel_width\n",
    "        if self.edit_channel_width % self.num_editing_heads != 0:\n",
    "            print(\"Error: config hidden size is not divisible by num_editing_heads\")\n",
    "        self.head_dim = self.edit_channel_width // self.num_editing_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        max_positions = (\n",
    "            config.max_position_embeddings\n",
    "        )  # does this do anything? can try killing this later\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(\n",
    "                torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
    "            ).view(1, 1, max_positions, max_positions),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # We compute Q and K as a single nn.linear; but will later break apart into subcomponents\n",
    "\n",
    "        ## Before modification to a variable channel-width\n",
    "        # self.q_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.v_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.q_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.k_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.v_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.out_proj = nn.Linear(self.edit_channel_width, self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_editing_heads, self.head_dim)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _new_reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Assume that we are doing softmax attention\n",
    "        # Project and split the query, key, value tensors\n",
    "        split_query = self._split_heads(query)\n",
    "        split_key = self._split_heads(key)\n",
    "        split_value = self._split_heads(value)\n",
    "\n",
    "        # Double-application (is this actually good/better for some reason?)\n",
    "        # self._split_heads(self.q_attn(query))\n",
    "        # self._split_heads(self.k_attn(key))\n",
    "        # self._split_heads(self.v_attn(value))\n",
    "\n",
    "        if split_query.dim() != 4:\n",
    "            print(\"Error: Expected query to be 4D tensor, but got something else!\")\n",
    "        if split_key.dim() != 3:\n",
    "            print(\"Error: Expected key to be 3D tensor, but got something else!\")\n",
    "        if split_value.dim() != 3:\n",
    "            print(\"Error: Expected value to be 3D tensor, but got something else!\")\n",
    "\n",
    "        # Query should be shaped as (batch_index, sequence_index, head_index, head_dim)\n",
    "        # Key and value should be shaped as (batch_index, head_index, head_dim)\n",
    "\n",
    "        # print(\n",
    "        #     \"SHAPES PRIOR TO ATTN CALC\",\n",
    "        #     split_query.permute(0, 2, 1, 3).shape,\n",
    "        #     split_key.unsqueeze(-1).shape,\n",
    "        #     split_value.unsqueeze(-1).shape,\n",
    "        # )\n",
    "\n",
    "        KQ_weights = torch.matmul(\n",
    "            split_query.permute(0, 2, 1, 3), split_key.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Then we take the softmax within the positional divisions\n",
    "        softmaxed_weights = nn.functional.softmax(KQ_weights, dim=-1)\n",
    "\n",
    "        # Adjusting value selection for head dimension\n",
    "        attn_output = torch.matmul(\n",
    "            softmaxed_weights.unsqueeze(-1), split_value.unsqueeze(-2)\n",
    "        )\n",
    "\n",
    "        # combine heads: change 50, 8, 104, 96 to 50, 104, 768\n",
    "        # first, permute\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3)\n",
    "\n",
    "        # combin heads x head_dims\n",
    "        attn_output = attn_output.reshape(\n",
    "            -1, attn_output.size(1), attn_output.size(2) * attn_output.size(3)\n",
    "        )\n",
    "        # now project back\n",
    "        projected_output = self.out_proj(attn_output)\n",
    "\n",
    "        return projected_output, softmaxed_weights\n",
    "\n",
    "    def _reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        if key.dim() == 4:\n",
    "            K_reduced = key[\n",
    "                :, :, -1, :\n",
    "            ]  # R# Check: that the second dimension of K is only a single element when we have batching\n",
    "            KQ_weights = torch.bmm(K_reduced, query.transpose(1, 2))\n",
    "            logistic_weights = torch.atan(KQ_weights)\n",
    "            attn_output = torch.bmm(\n",
    "                logistic_weights.transpose(1, 2),\n",
    "                value[\n",
    "                    :, :, -1, :\n",
    "                ],  # we take the editor output only over the final token position\n",
    "            )\n",
    "\n",
    "        if key.dim() == 3:\n",
    "            QK_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "            logistic_weights = torch.atan(QK_weights)\n",
    "            attn_output = torch.matmul(logistic_weights, value)\n",
    "\n",
    "        return attn_output, logistic_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_hidden_states,\n",
    "        target_hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # Here, the query is the target hidden encoder, the key is the editor, and the value is the editor\n",
    "        query = self.q_attn(target_hidden_states)\n",
    "        if editor_hidden_states.dim() == 3:\n",
    "            key = self.k_attn(\n",
    "                # I don't quite understand why sometimes editor_hidden_states is 4 dimensional, sometimes 3\n",
    "                # seems like it's sometimes 20, 1, 4, 768 and sometimes 20, 4, 768. what gives?\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        if editor_hidden_states.dim() == 4:\n",
    "            key = self.k_attn(\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        # print(\n",
    "        #     \"q_attn weight/bias norm\",\n",
    "        #     self.q_attn.weight.norm(),\n",
    "        #     self.q_attn.bias.norm(),\n",
    "        #     self.q_attn.weight.shape,\n",
    "        # )\n",
    "        # print(\n",
    "        #     \"k_attn weight/bias norm\",\n",
    "        #     self.k_attn.weight.norm(),\n",
    "        #     self.k_attn.bias.norm(),\n",
    "        # self.k_attn.weight.shape,\n",
    "        # )\n",
    "        # print(\n",
    "        #     \"v_attn weight/bias norm\",\n",
    "        #     self.v_attn.weight.norm(),\n",
    "        #     self.v_attn.bias.norm(),\n",
    "        #     self.v_attn.weight.shape,\n",
    "        # )\n",
    "        # print(\n",
    "        #     #     \"Q/K/V norm (old):\",\n",
    "        #     #     query.norm(),\n",
    "        #     #     key.norm(),\n",
    "        #     #     value.norm(),\n",
    "        #     query.shape,\n",
    "        #     key.shape,\n",
    "        #     value.shape,\n",
    "        # )\n",
    "\n",
    "        attn_output, attn_weights = self._new_reverse_attn(query, key, value)\n",
    "\n",
    "        # print(\"ATTN OUT SHAPE\", attn_output.shape)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    # labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    # print(\"HIDDEN STATE SHAPE\", hidden_states.shape)\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "    # lm_logits = self.lm_head(hidden_states)\n",
    "    reverse_attention_output = self.lm_head(\n",
    "        hidden_states, encoder_hidden_states, output_attentions=output_attentions\n",
    "    )\n",
    "\n",
    "    # print(\"REVERSE ATTENTION OUTPUT SHAPE\", reverse_attention_output[0].shape)\n",
    "\n",
    "    return reverse_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_final_layer_with_bespoke_reverse_attention(model):\n",
    "    model.lm_head = Editor_Attention(config=model.config)\n",
    "    model.forward = new_forward.__get__(model, GPT2LMHeadModel)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def add_fwd_hooks(module_hooks):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for mod, hk in module_hooks:\n",
    "            handles.append(mod.register_forward_hook(hk))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer_indices(model):\n",
    "    \"\"\"\n",
    "    Assigns a custom attribute 'layer_index' to each transformer layer in the GPT-2 model.\n",
    "    This function iterates over the transformer blocks and assigns an index to each.\n",
    "    \"\"\"\n",
    "    model.transformer.wte.layer_index = 0\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        layer.layer_index = i + 1\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# assign_layer_indices(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_pad(A, B):\n",
    "    batch_size, n_tokens_A = A.size()\n",
    "    n_tokens_B = B.size(1)\n",
    "\n",
    "    # Find the lengths in A and B\n",
    "    lengths_A = torch.sum(A != 50256, dim=1)\n",
    "    lengths_B = torch.sum(B != 50256, dim=1)\n",
    "\n",
    "    # iniitalize empty tensor\n",
    "    result = torch.full(\n",
    "        (\n",
    "            batch_size,\n",
    "            max(lengths_A + lengths_B),\n",
    "        ),\n",
    "        50256,\n",
    "    )\n",
    "\n",
    "    # Concatenate A[i] and B[i] a\n",
    "    for i in range(batch_size):\n",
    "        result[i, : lengths_A[i]] = A[i, : lengths_A[i]]\n",
    "        result[i, lengths_A[i] : lengths_A[i] + lengths_B[i]] = B[i, : lengths_B[i]]\n",
    "\n",
    "    # # Concatenate A and B along dimension 1\n",
    "    # concatenated = torch.cat((A, B), dim=1)\n",
    "\n",
    "    # # Find the number of non-zero elements in each row of concatenated tensor\n",
    "    # lengths = torch.sum(concatenated != 50256, dim=1)\n",
    "\n",
    "    # # Create a mask to identify the non-zero elements in concatenated tensor\n",
    "    # mask = torch.arange(n_tokens_A + n_tokens_B).expand(batch_size, n_tokens_A + n_tokens_B) < lengths.unsqueeze(1)\n",
    "\n",
    "    # # Create a new tensor with the same shape as concatenated tensor\n",
    "    # result = torch.full_like(concatenated, 50256)\n",
    "\n",
    "    # # Move the non-zero elements to the left and zero elements to the right\n",
    "    # result[mask] = concatenated[mask]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN values found in gradient of parameter: {name}\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "class EditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Construct Editor Model\n",
    "        # Load the configuration from the YAML file\n",
    "        # with open(editor_yaml_file_path, 'r') as file:\n",
    "        #     self.config = yaml.safe_load(file)\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model = (\n",
    "                GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )  # have recently added .cuda() so it uses the gpu\n",
    "        else:\n",
    "            self.editor_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"mps\").eval()\n",
    "\n",
    "        # Add cross-attention to each layer\n",
    "        self.editor_model.config.add_cross_attention = True\n",
    "        self.editor_model.config.num_editing_heads = num_editing_heads\n",
    "        self.editor_model.config.chop_layer = chop_editor_at_layer\n",
    "        self.editor_model.config.kill_token_zero = kill_token_zero\n",
    "        self.editor_model.config.edit_channel_width = edit_channel_width\n",
    "\n",
    "        if chop_editor_at_layer is None:\n",
    "            chop_editor_at_layer = 12\n",
    "\n",
    "        for i in range(chop_editor_at_layer):\n",
    "            add_cross_attention_to_layer(\n",
    "                self.editor_model.transformer.h[i], self.editor_model.config\n",
    "            )\n",
    "\n",
    "        # Delete extra layers beyond the chop_layer\n",
    "        self.editor_model.transformer.h = self.editor_model.transformer.h[\n",
    "            :chop_editor_at_layer\n",
    "        ]\n",
    "\n",
    "        # Replace the final linear layer with special reverse attention output\n",
    "        self.editor_model.lm_head = Editor_Attention(config=self.editor_model.config)\n",
    "        self.editor_model.forward = new_forward.__get__(\n",
    "            self.editor_model, GPT2LMHeadModel\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model.cuda()\n",
    "        else:\n",
    "            self.editor_model.to(\"mps\")\n",
    "\n",
    "        # Construct Target Model\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )\n",
    "        else:\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "                .to(\"mps\")\n",
    "                .eval()\n",
    "            )\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        assign_layer_indices(self.target_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model.cuda()\n",
    "        else:\n",
    "            self.target_model.to(\"mps\")\n",
    "\n",
    "        # Add module for layerwise embeddings\n",
    "        if use_layerwise_embeddings:\n",
    "            self.use_layerwise_embeddings = True\n",
    "            self.layerwise_embeddings = torch.randn(13, 768, requires_grad=True).to(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "            )\n",
    "        else:\n",
    "            self.use_layerwise_embeddings = False\n",
    "            self.layerwise_embeddings = 0\n",
    "\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lossfn = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "\n",
    "    # Gets the hidden states from the target model, if necessary\n",
    "    def run_target_model_for_encoded_hidden_states(self, target_ids):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.target_model(target_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids,\n",
    "        target_input_ids,\n",
    "        target_hidden_states=None,\n",
    "        output_target_hidden_states=False,\n",
    "        output_edited_hidden_states=False,\n",
    "        output_edit_vectors=False,\n",
    "        output_editor_attention=False,\n",
    "        stop_editing_index=None,\n",
    "        batch_edit_vectors=None,\n",
    "    ):\n",
    "        # Run target model for encoded hidden states\n",
    "        if target_hidden_states is None:\n",
    "            target_hidden_states = torch.stack(\n",
    "                self.run_target_model_for_encoded_hidden_states(\n",
    "                    target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "                ),  # seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\n",
    "                dim=2,\n",
    "            )\n",
    "        # dimensions of target_hidden_states:\n",
    "        # batch_size, token_sequence_length, num_layers = 13, resid_width = 768\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index, then we eliminate target_hidden_states beyond that index\n",
    "        if stop_editing_index is not None:\n",
    "            target_hidden_states = target_hidden_states[\n",
    "                :, :stop_editing_index, :, :\n",
    "            ].clone()\n",
    "\n",
    "        # Normalize along the last dimension\n",
    "        normalization_factors = target_hidden_states.norm(dim=-1, keepdim=True)\n",
    "        target_hidden_states = target_hidden_states / normalization_factors\n",
    "\n",
    "        # Error catching:\n",
    "        if batch_edit_vectors is not None:\n",
    "            if output_edit_vectors or output_editor_attention:\n",
    "                return \"Error: Inputting your own batch_edit_vectors means the model does not construct the outputs you are requesting\"\n",
    "\n",
    "        # Run editor model, get edit vectors\n",
    "        if batch_edit_vectors is None:\n",
    "            if self.use_layerwise_embeddings:\n",
    "                # Now, add in the layerwise embeddings\n",
    "                embedded_hidden_states = (\n",
    "                    target_hidden_states + self.layerwise_embeddings[None, None, :, :]\n",
    "                )\n",
    "\n",
    "                collapsed_target_hidden_states = embedded_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "            else:\n",
    "                collapsed_target_hidden_states = target_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "\n",
    "            # print(\"EDITOR INPUT ID SHAPE\", editor_input_ids.shape)\n",
    "            # print(\"TARGET HIDDEN SHAPE\", target_hidden_states.shape)\n",
    "\n",
    "            editor_output = self.editor_model(\n",
    "                editor_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                encoder_hidden_states=collapsed_target_hidden_states,\n",
    "                output_attentions=output_editor_attention,\n",
    "            )\n",
    "            # Multiply the outputs by normalization factors\n",
    "            if output_editor_attention:\n",
    "                temp_edit_vectors = editor_output[0]\n",
    "                # Might want to reshape this too but whatever\n",
    "                batch_editor_attention = editor_output[1]\n",
    "            else:\n",
    "                temp_edit_vectors = editor_output\n",
    "\n",
    "            # print(\"TEMP EDIT VECTORS SHAPE\", temp_edit_vectors.shape)\n",
    "\n",
    "            # Renormalize to the scale of the target hidden states\n",
    "            # and reshape to proper dimensions\n",
    "            batch_edit_vectors = (\n",
    "                self.edit_dampening_factor\n",
    "                * normalization_factors\n",
    "                * temp_edit_vectors.reshape(\n",
    "                    temp_edit_vectors.shape[0], stop_editing_index, 13, 768\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index,\n",
    "        # this pads batch_edit_vectors with 0's to the right of the edited positions\n",
    "        if stop_editing_index is not None:\n",
    "            batch_edit_vectors = torch.cat(\n",
    "                (\n",
    "                    batch_edit_vectors,\n",
    "                    torch.zeros(\n",
    "                        batch_edit_vectors.shape[0],\n",
    "                        target_input_ids.shape[1] - stop_editing_index,\n",
    "                        13,\n",
    "                        768,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Run target model with edit vectors. This adds the edit vectors to the given hidden state at the specified batch index, position, and layer\n",
    "        def edit_add(module, input, output):\n",
    "            layer_index = module.layer_index\n",
    "            output[0][:] = output[0] + batch_edit_vectors[:, :, layer_index, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[0][:, 0, :] = 0\n",
    "\n",
    "        def embedding_edit_add(module, input, output):\n",
    "            output[:] = output + batch_edit_vectors[:, :, 0, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[:, 0, :] = 0\n",
    "\n",
    "        # Now editing the target model\n",
    "        hooks1 = [(self.target_model.transformer.wte, embedding_edit_add)]\n",
    "        hooks2 = [(self.target_model.transformer.h[L], edit_add) for L in range(12)]\n",
    "        hooks = hooks1 + hooks2\n",
    "        with add_fwd_hooks(hooks):\n",
    "            # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n",
    "            # THE END OF `layer` AS A SIDE EFFECT)\n",
    "            target_result = self.target_model(\n",
    "                target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                output_hidden_states=output_edited_hidden_states,\n",
    "            )\n",
    "\n",
    "        logits = target_result.logits\n",
    "\n",
    "        output = {}\n",
    "        output[\"logits\"] = logits\n",
    "        if output_target_hidden_states:\n",
    "            output[\"target_hidden_states\"] = (\n",
    "                target_hidden_states * normalization_factors\n",
    "            )\n",
    "        if output_edited_hidden_states:\n",
    "            output[\"edited_hidden_states\"] = target_result.hidden_states\n",
    "        if output_edit_vectors:\n",
    "            output[\"edit_vectors\"] = batch_edit_vectors\n",
    "        if output_editor_attention:\n",
    "            output[\"editor_attention\"] = batch_editor_attention\n",
    "        return output\n",
    "\n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "            self.editor_inputs = batch[\"tokenized_first_sentence\"][i].unsqueeze(0)\n",
    "            self.target_inputs = batch[\"tokenized_next_50_tokens\"][i].unsqueeze(0)\n",
    "            self.prediction = self.forward(\n",
    "                self.editor_inputs,\n",
    "                self.target_inputs,\n",
    "                stop_editing_index=stop_editing_index,\n",
    "                output_target_hidden_states=False,\n",
    "                output_edited_hidden_states=False,\n",
    "                output_edit_vectors=False,\n",
    "                output_editor_attention=False,\n",
    "            )\n",
    "            # compute most likely tokens from the logits\n",
    "            predicted_ids = [\n",
    "                torch.argmax(pred, dim=-1) for pred in self.prediction[\"logits\"]\n",
    "            ]\n",
    "            # convert the token ids to strings\n",
    "            predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "            return predicted_strings\n",
    "\n",
    "    def evaluate_KL_test_loss_nogradient(\n",
    "        self, dataloader, f_data_to_soft_labels=None, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                # Now we must compute the soft labels!\n",
    "                # join the last 50 tokens to the editor inputs\n",
    "                soft_labels = f_data_to_soft_labels(\n",
    "                    batch[\"tokenized_first_sentence\"],\n",
    "                    batch[\"tokenized_next_50_tokens\"],\n",
    "                    num_predictions_max=50,\n",
    "                )\n",
    "                mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                self.loss = torch.nn.functional.kl_div(\n",
    "                    log_prob_predictions[mask, :],\n",
    "                    soft_labels[mask, :],\n",
    "                    reduction=\"batchmean\",\n",
    "                )\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += self.loss.item() * current_batch_size\n",
    "                total_samples += current_batch_size\n",
    "            weighted_average_loss = sum_weighted_losses / total_samples\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def evaluate_crossentropy_test_loss_nogradient(\n",
    "        self, dataloader, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_tokens = 0\n",
    "            for batch in dataloader:\n",
    "                # current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Create a mask to exclude padded tokens\n",
    "                target_labels = self.target_inputs[:, stop_editing_index:].reshape(-1)\n",
    "                mask = (\n",
    "                    target_labels != 50256\n",
    "                )  # Assuming padded tokens are represented by 0\n",
    "\n",
    "                # Compute the cross-entropy loss with masking\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = criterion(log_prob_predictions, target_labels)\n",
    "                current_mask_sum = mask.sum()\n",
    "                loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += loss * current_mask_sum\n",
    "                total_tokens += current_mask_sum\n",
    "            weighted_average_loss = sum_weighted_losses / total_tokens\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def run_train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "    ):\n",
    "        self.opt = optim.AdamW(\n",
    "            self.parameters(), lr=lr, weight_decay=0.01\n",
    "        )  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        if KL_divergence_loss:\n",
    "            self.lossfn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        else:\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "                disable=True,\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for step, batch in enumerate(\n",
    "                    train_loader\n",
    "                ):  # not sure what this does for fractional batches. meh whatev\n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"tokenized_next_50_tokens\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.prediction = self.forward(\n",
    "                        batch[\"tokenized_first_sentence\"],\n",
    "                        batch[\"tokenized_next_50_tokens\"],\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                        output_target_hidden_states=True,\n",
    "                        output_edited_hidden_states=True,\n",
    "                        output_edit_vectors=True,\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "                    \n",
    "                    # Compute the data loss\n",
    "                    if KL_divergence_loss:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "                        self.soft_labels = f_data_to_soft_labels(\n",
    "                            batch[\"tokenized_first_sentence\"],\n",
    "                            batch[\"tokenized_next_50_tokens\"],\n",
    "                            num_predictions_max=50,\n",
    "                        )\n",
    "                        # check that the mask makes sense!\n",
    "                        mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                        self.prediction_loss = self.lossfn(\n",
    "                            log_prob_predictions[mask, :], self.soft_labels[mask, :]\n",
    "                        )\n",
    "                        # NOTE: currently I am letting the loss predict on tokens inside the editing window\n",
    "                        # I didn't do this in the previous testing! Nor is it the case in crossentropy\n",
    "\n",
    "                    else:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][\n",
    "                                :, stop_editing_index:, :\n",
    "                            ].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Create a mask to exclude padded tokens\n",
    "                        # NOTE: here we are disallowing prediction on the first stop_editing_index tokens.\n",
    "                        # Code is currently formatted such that this is not the case for KL\n",
    "                        target_labels = batch[\"tokenized_next_50_tokens\"][\n",
    "                            :, stop_editing_index:\n",
    "                        ].reshape(-1)\n",
    "                        mask = target_labels != 50256\n",
    "                        # Compute the cross-entropy loss with masking\n",
    "                        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                        loss = criterion(log_prob_predictions, target_labels.long())\n",
    "                        current_mask_sum = mask.sum()\n",
    "                        self.prediction_loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "\n",
    "                    metrics = {\n",
    "                        \"step\": step * (epoch + 1),\n",
    "                        \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                        \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                        \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                        \"train_batch_gradient_norm\": gradient_norm,\n",
    "                    }\n",
    "\n",
    "                    if wandb.run:\n",
    "                        wandb.log(metrics)\n",
    "                    if step % 100 == 0:\n",
    "                        print(metrics)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                    if current_time - last_checkpoint_time >= checkpoint_interval * 60:\n",
    "                        # Save the checkpoint\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            f\"checkpoint_epoch_{epoch}_batch_{pbar.n}.pt\",\n",
    "                        )\n",
    "                        last_checkpoint_time = current_time\n",
    "                        # announce checkpoint save\n",
    "                        print(\"Checkpoint saved at epoch\", epoch, \"batch\", pbar.n)\n",
    "\n",
    "                ####END BATCH LOOP\n",
    "                #########################\n",
    "\n",
    "                # epoch loss\n",
    "                # epoch_test_prediction_loss = self.evaluate_crossentropy_test_loss_nogradient(\n",
    "                #     test_loader,\n",
    "                #     stop_editing_index,\n",
    "                #     batch_size\n",
    "                # )\n",
    "                if KL_divergence_loss:\n",
    "                    epoch_test_prediction_loss = self.evaluate_KL_test_loss_nogradient(\n",
    "                        dataloader=test_loader,\n",
    "                        f_data_to_soft_labels=f_data_to_soft_labels,\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                    )\n",
    "\n",
    "                # # Calculate and accumulate gradient norm for logging\n",
    "                # gradients = [p.grad.view(-1) for p in self.parameters() if p.grad is not None]\n",
    "                # all_gradients = torch.cat(gradients)\n",
    "                # gradient_norm = torch.norm(all_gradients).item()\n",
    "                # epoch_gradient_norm += gradient_norm\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"test_prediction_loss\": epoch_test_prediction_loss,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self, \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DIVISION!\n",
    "# DATASET CONSTRUCTION BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV file into a DataFrame\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "csv_dir = \"../assets/data/wikipedia_three_sentences.csv\"\n",
    "dataframe_dir = \"wikipedia_df.pt\"\n",
    "\n",
    "\n",
    "def tokenize_followup(tokenizer, row):\n",
    "    # Compose second_sentences and third_sentences\n",
    "    followup_text = row[\"second_sentences\"] + \" \" + row[\"third_sentences\"]\n",
    "\n",
    "    # Select the first 500 characters\n",
    "    followup_text = followup_text[:500]\n",
    "\n",
    "    # Tokenize the followup text\n",
    "    tokenized_followup = tokenizer.encode(followup_text)\n",
    "\n",
    "    # Check if the resulting tokenized list is still less than 50 tokens\n",
    "    if len(tokenized_followup) < 50:\n",
    "        # Pad with token 50256 to reach a length of 50 tokens\n",
    "        tokenized_followup = tokenized_followup + [50256] * (\n",
    "            50 - len(tokenized_followup)\n",
    "        )\n",
    "\n",
    "    return tokenized_followup\n",
    "\n",
    "\n",
    "def create_dataframe_from_csv(csv_dir):\n",
    "    df = pd.read_csv(csv_dir)\n",
    "\n",
    "    # # Printng out examples of longest and shortest entries\n",
    "    # # Iterate over each column\n",
    "    # for column in df.columns:\n",
    "    #     print(f\"Column: {column}\")\n",
    "\n",
    "    #     # Find the 10 longest entries in the column\n",
    "    #     longest_entries = df[column].astype(str).apply(len).nlargest(10)\n",
    "    #     print(\"Longest entries:\")\n",
    "    #     for index, length in longest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     # Find the 10 shortest entries in the column\n",
    "    #     shortest_entries = df[column].astype(str).apply(len).nsmallest(10)\n",
    "    #     print(\"Shortest entries:\")\n",
    "    #     for index, length in shortest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     print()\n",
    "\n",
    "    # Length of the dataframe to start\n",
    "    # print(f\"Length of dataframe: {len(df)}\")\n",
    "    # df.columns\n",
    "    # Filter the dataset based on sentence length\n",
    "\n",
    "    df[\"first_sentence_length\"] = df[\"first_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"second_sentence_length\"] = df[\"second_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"third_sentence_length\"] = df[\"third_sentences\"].apply(lambda x: len(x))\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df[\"first_sentence_length\"] >= 5)\n",
    "        & (df[\"second_sentence_length\"] >= 10)\n",
    "        & (df[\"first_sentence_length\"] <= 100)\n",
    "    ]\n",
    "    # print(f\"Length of filtered dataframe: {len(df_filtered)}\")\n",
    "\n",
    "    from transformers import GPT2Tokenizer\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    df_filtered[\"tokenized_first_sentence\"] = df_filtered.apply(\n",
    "        lambda row: tokenizer.encode(row[\"first_sentences\"], add_special_tokens=False),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_filtered[\"tokenized_next_50_tokens\"] = df_filtered.apply(\n",
    "        lambda row: partial(tokenize_followup, tokenizer)(row), axis=1\n",
    "    )\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "if os.path.exists(\"wikipedia_df.pt\"):\n",
    "    df = pd.read_pickle(\"wikipedia_df.pt\")\n",
    "else:  # save dataset\n",
    "    df = create_dataframe_from_csv(csv_dir)\n",
    "    df.to_pickle(\"wikipedia_df.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    res = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k != \"__index_level_0__\":\n",
    "            els = [x[k] for x in batch]\n",
    "            max_length = max(len(x) for x in els)\n",
    "            if k == \"tokenized_next_50_tokens\":\n",
    "                max_length = 50\n",
    "                for i in range(len(els)):\n",
    "                    if len(els[i]) > 50:\n",
    "                        els[i] = els[i][:50]\n",
    "            # for x in els:\n",
    "            #     x += [0] * (max_length - len(x))\n",
    "            # res[k] = torch.cat([y , torch.zeros(max_length - len(x))],for y in els)\n",
    "            # res[k]=torch.tensor([x + [0] * (max_length( - len(x)) for x in els], dtype=torch.long)\n",
    "            res[k] = torch.stack(\n",
    "                [\n",
    "                    torch.cat(\n",
    "                        (torch.tensor(x), torch.full((max_length - len(x),), 50256))\n",
    "                    )\n",
    "                    for x in els\n",
    "                ]\n",
    "            ).int()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 987633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 85882\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 2 columns to Dataset and train-test split\n",
    "dataset = Dataset.from_pandas(\n",
    "    df[[\"tokenized_first_sentence\", \"tokenized_next_50_tokens\"]]\n",
    ").shuffle(seed=42)\n",
    "test_ratio = 0.08\n",
    "temp = dataset.train_test_split(test_size=test_ratio)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # 50 or so\n",
    "data_loader = DataLoader(\n",
    "    temp[\"train\"], batch_size=batch_size, collate_fn=collate_fn\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(\n",
    "    temp[\"test\"], batch_size=batch_size, collate_fn=collate_fn\n",
    ")  # batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# # Now you can iterate over data_loader in your training loop\n",
    "# j=0\n",
    "# for batch in data_loader:\n",
    "#     j=j+1\n",
    "# print(j)\n",
    "# #Good! we have all the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "     num_rows: 1073515\n",
       " }),\n",
       " 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, data_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f85bdbedde0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# # temp[\"tokenized_next_50_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_data_to_soft_labels(A, B, num_predictions_max=50):\n",
    "    with torch.no_grad():\n",
    "        # Compute number of nonzero elements in each row of A and B\n",
    "        lengths_A = torch.sum(A != 50256, dim=1)\n",
    "        lengths_B = torch.sum(B != 50256, dim=1)\n",
    "        # Concatenate A and B along dimension 1\n",
    "        data = concat_and_pad(A, B)\n",
    "        logits = model(data).logits\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=2)\n",
    "\n",
    "        # Create an empty tensor to store the predictions\n",
    "        shape = (len(lengths_A), num_predictions_max, 50257)\n",
    "        hold_output = torch.full(shape, torch.nan)\n",
    "\n",
    "        # Extract the predictions corresponding to B\n",
    "        for i in range(len(lengths_A)):\n",
    "            hold_output[i, : lengths_B[i], :] = predictions[\n",
    "                i, lengths_A[i] : lengths_A[i] + lengths_B[i], :\n",
    "            ]\n",
    "\n",
    "        return hold_output.reshape(-1, 50257)  # returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_editing_index = 8\n",
    "hypernetwork = EditorHypernetwork(\n",
    "    edit_dampening_factor=edit_dampening_factor,  # 1/10000,\n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EditorHypernetwork(\n",
       "  (editor_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-5): 6 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (crossattention): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (q_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Editor_Attention(\n",
       "      (q_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (k_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (v_attn): Linear(in_features=768, out_features=1536, bias=True)\n",
       "      (out_proj): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.gpt2 import GPT2Editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in data_loader:\n",
    "    if i == 18:\n",
    "        temp = batch\n",
    "        break\n",
    "    i += 1\n",
    "temp[\"tokenized_next_50_tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokenized_first_sentence', 'tokenized_next_50_tokens'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 35]), torch.Size([32, 50]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    batch_sample[\"tokenized_first_sentence\"].shape,\n",
    "    batch_sample[\"tokenized_next_50_tokens\"].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everett Township is one of twelve townships in Burt County, Nebraska, United States.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_first_sentence\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The population was 1,149 at the 2000 census. A 2006 estimate placed the township's population at 1,072..\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_next_50_tokens\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing the forward pass on temp\n",
    "# log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "#     hypernetwork.forward(\n",
    "#         temp[\"tokenized_first_sentence\"],\n",
    "#         temp[\"tokenized_next_50_tokens\"],\n",
    "#         stop_editing_index=stop_editing_index,\n",
    "#         output_target_hidden_states=True,\n",
    "#         output_edited_hidden_states=True,\n",
    "#         output_edit_vectors=True,\n",
    "#     )[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "#     dim=1,\n",
    "# )\n",
    "# soft_labels = f_data_to_soft_labels(\n",
    "#     temp[\"tokenized_first_sentence\"],\n",
    "#     temp[\"tokenized_next_50_tokens\"],\n",
    "#     num_predictions_max=50\n",
    "# )\n",
    "# mask = (temp[\"tokenized_next_50_tokens\"] != 0).reshape(-1)\n",
    "# torch.where(torch.isnan(soft_labels)[:,0] & mask)\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"].reshape(-1)[650:700])\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"][12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized_first_sentence': tensor([[   36, 33395, 32115,  ..., 50256, 50256, 50256],\n",
      "        [18454,   519,  8152,  ..., 50256, 50256, 50256],\n",
      "        [16305,  9271,   373,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [    1,  7120,  5438,  ..., 50256, 50256, 50256],\n",
      "        [  464,   370,    13,  ..., 50256, 50256, 50256],\n",
      "        [46509,   805,   318,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[  464,  3265,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 1065,  2425,  1906,  ..., 50256, 50256, 50256],\n",
      "        [ 1544, 14131, 11161,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1026, 18530,  1737,  ..., 50256, 50256, 50256],\n",
      "        [    5, 10062, 13485,  ...,    11,   543,   373],\n",
      "        [ 1026,   318, 22765,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'step': 0, 'train_batch_total_loss': 0.4781807065010071, 'train_batch_prediction_loss': 0.4781807065010071, 'train_batch_penalty_loss': tensor(0., device='cuda:0', grad_fn=<MeanBackward0>), 'train_batch_gradient_norm': 0.00717251468449831}\n",
      "Checkpoint saved at epoch 0 batch 0\n",
      "{'tokenized_first_sentence': tensor([[   36,    83,  1359,  8607, 23453,  1045,   318,   257,   937,   420,\n",
      "           313,    88,   992,   261,   516,  4618,  4693,  3417,   416,  7576,\n",
      "         46892,  3873,   265,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 5840,   418,   709,  7499,   318,   257,  3394, 27543, 40358,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [45896,  3841,  3334,  3961,   318,   257,  1171,  1029,  1524,   287,\n",
      "           262,  3240,   286, 41540,    11, 10140,    11,  1578,  1829,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10082,  2364,  4364,  5199, 26618,   357,  6286,  2534,  2901, 22717,\n",
      "             8,   318,   257,  1966,  6638,  1067,   624,  2357,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24035, 32653, 22132,   318,   257, 37547,  1605, 10997,  2646, 20495,\n",
      "         13085,    64, 22937,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 33530, 26006, 25323,  3719,  8228,  9327,    11,  1766,   261,\n",
      "          2675,    11,   373,  4920,   416,   262,  3517, 10964,    83,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [43223,   286,   262,  8662,   318,   262, 16974,  8034,  5062,   416,\n",
      "          5398,    12,  7437,  3881,  4097,  2441,   381,   268, 18829,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2782,  4484,  3362, 25846,   357,  6286,  1367,  3269, 20510,     8,\n",
      "           318,   281,  3594,  4346,  4706,   290,  1966,  2137,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   49, 18013,  1610,   454,   318,   257,  7525,   257, 11913,  2057,\n",
      "            11,   925,   503,   286,   371, 18013, 13020,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24119,  1044,  1849, 43401,  1849, 22370,   357, 12740,  1849, 22370,\n",
      "             8,   318,   257,  1181, 12763,   287,   262,   471,    13,    50,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 8001,   794, 22935, 16220,   290, 32490,  1134,    25, 12466,   238,\n",
      "         21169, 20375, 35072,   141,   229,     8,   318,   257,  7404,   287,\n",
      "          1778,   456,    67, 17718,    11,  7840, 32490,  1134,  4103,    13],\n",
      "        [  464, 10727,   290,  4885, 36289,   220,   318,   257,   220,  5016,\n",
      "          2873, 24337,  7351,   262,   471,    13,    50,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  9369,   318,   281,  1605, 10997,    12,    67, 20058, 11305,\n",
      "          5581,  2168,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,   968,  8936,  3478,    12, 22569,  3465,   318,   257,   968,\n",
      "          8936,  3331, 11295,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [16286,    88,   347,  3979,   318,   257, 16236, 14469, 14266,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10227, 13374, 29228,   318,   257,  7872,    12, 11186,   289,  1739,\n",
      "          3491,   287,   262,  7840,  2498,   931,  6192, 38712,   286, 37935,\n",
      "            64,  8386,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32,  8034,  6256, 39588,   318,   257,  9566,  2615,   290, 21421,\n",
      "          5140,   379,  4434,   455, 26084,   365, 17871,   395,    72,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2750,  2536, 35803,   726,    68,  6164,   318,   530,   286,\n",
      "           262,  4387,  3869, 18446,   287,  3284,   290,   287,   262,   995,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 8248,   785,    25,   383,   471,    13,    50,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   39,  4131,  3686, 49696,   457,    65, 15386, 39891,   357,   397,\n",
      "          4679,    85,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 34424,  4897,   286, 12495, 10982,   416,  6926,   318,   257,\n",
      "         26589, 43469,   286,  3923, 13012,   416,   317,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2351, 10006,   318,   262,  2793, 11847,   286, 17871, 41145,\n",
      "           338,   275,   291,   321,  1691,  8411,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [14942,  1044,  3334,  3961,   318,   257,  1171,  1029,  1524,   351,\n",
      "          2444,   287, 19051,  5193,   832, 14104,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   52,   311,   337,   453,    64,  1423,  2675, 10499,   318, 22765,\n",
      "           379, 27609, 40612,   287,  3794,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   33,   418,  8461,  4730,   389,   281,  9450,  1448,   287, 50008,\n",
      "           305,    11,   717,  5495,   287,   262,  5816, 21649,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7447, 24408,  3248, 32790, 21862,    72, 38325,   318,   257, 50008,\n",
      "         12769,  6260,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,   317,  2701,    64,   402,   710,   747,   318,   257,  5680,\n",
      "           282,   578,   308,   710,   747,   287,   262, 38795,  1067, 13951,\n",
      "           287, 20572, 42354,    11,  3340,    13, 50256, 50256, 50256, 50256],\n",
      "        [   51, 30915,  3996, 24683,   393,  2391, 15585, 24683,   318,   257,\n",
      "         36784,  5625,   393,  6589,   656,   257,  7779,  3996,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   53,   343,    70,   571,   330, 44342,  5466,    87,   328,   641,\n",
      "           318,   257,  4693,   286, 20159,    12, 24561, 11492,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [13195,   903,  1122,   318,   257,  7404,   290,  3026, 28830,   287,\n",
      "           262,   775,  1940,   268,  5665,   286,  3687, 41588,    11,  4492,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 3109,  2655,  1219,   346,   388,  1885,   359,   272,   388,   318,\n",
      "           257,  4693,   286, 39526,   287,   262,  1641, 18063,   418,  1819,\n",
      "         48319,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [44203,   337,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[   36,    83,  1359,  ..., 50256, 50256, 50256],\n",
      "        [20459, 23564, 11283,  ..., 50256, 50256, 50256],\n",
      "        [ 1026,   318, 12228,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [22005,   663, 22303,  ..., 50256, 50256, 50256],\n",
      "        [21077,   287, 14159,  ..., 50256, 50256, 50256],\n",
      "        [10438, 21444,   710,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[   41,  7780,   695,  3178,  5439,  3691,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22308,  1370, 38613,   318,   257, 43251,   416,  1014,   287, 42148,\n",
      "          1071,    11,  8919,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [48523,   504,  4244,   318,   281,   555,  1939, 40132,  2055,   287,\n",
      "           262, 23975, 32115,   286,  6065,   296,   500,    68,  3418,   287,\n",
      "           262,   471,    13,    50,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [33055,  1872,  4267, 33743,   318,   257, 34306,   286,  4898,    75,\n",
      "          1076,  1900,   422,   262, 13708,    11, 12281,    64,   399,  9019,\n",
      "            11,   290, 18133, 31656,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22788,  5658, 46039,   286,  1004,   893,    11,   513,  4372, 21770,\n",
      "           316,    11,   357,  6888,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 9414,  1726,    72,  4380,   338,  3615,   357, 15029,   323,    25,\n",
      "          2142,    72,   371, 15492,   265, 46070,    72,    11,  4810,    33,\n",
      "             8,   318,   257,  9301,  1964,  2151,   287, 46070,    72,    13],\n",
      "        [   47, 30102, 45895,  4910,   357, 47710,     8,   318,   262, 19646,\n",
      "          8034,  5062,   416,  9663, 14015,  1897,   316,  9084,   282, 30102,\n",
      "            77,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [40936,   292,   648,   859,   318,   257,  4793,  3942, 14964,  7344,\n",
      "            12, 16129,  2646,  7924,   416, 26105, 45051,  3208,   270,   917,\n",
      "            84,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   33,  8045,  1215,   349, 36335, 20492, 32839,  5302,   455,   641,\n",
      "           220,   357,  1954,  2693, 49658,   784,  2310,  2901, 31953,   828,\n",
      "           373,   257, 21402, 28278,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    1,  3237,   262,  5896,   287,   262,  2159,     1,   318,   257,\n",
      "          3496,   416, 29628,   710, 49398,    11,  2716,   355,   257,  2060,\n",
      "           287, 14489,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [31709,  4862,  3146,   287,  1578,  4498, 24880,   357,    52, 14242,\n",
      "             8,  1061,   257,  4838, 11426, 47622,  1410,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1858,   338, 13742, 19219,  1550,   318,   262,  1218,  8034,  5062,\n",
      "           416,   262,  3881,  4097, 14801, 16944,    11,  2716,   287,  7795,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 5781,  3861,   287, 17219,   318,   281, 28873, 47625,   329,   366,\n",
      "          9328,    75, 12057,   291, 30021,    12, 38301, 25897,  1911, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    1, 10044,   292,   578, 11397,     1,   318,   257,  3783, 10165,\n",
      "          1790,  1621,   416,  1605,  6260, 14167,   402,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  6244, 15244,  7458,   314,    12,    32,  4346,  1622,  4444,\n",
      "           351,   257,  4274, 17820,  2260, 12184,   983,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  1708,   318,   257,  1351,   286,  4056,  7025,   416,  8519,\n",
      "         11102,   357, 39190,   737, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [13949,   311,    19,   286, 22691, 15710, 12950, 22325,  7499,   481,\n",
      "           307,   257,  5093,  1906, 35782,  5801, 11168,  4911,  1627,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 3041,   303,   282,   318,   262,  8886,  8034,  5062,   416,  2520,\n",
      "          6983,  2933,  1448,   262,  6387,    89,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [14304,  1424,  1406,  4599,   318,   257, 15589, 21274,  5062,  2716,\n",
      "           416, 14663, 27609,  7935,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2558,  4873,    66,   309,  2943,    12,  1828,   318,   257,\n",
      "         10663,    12, 37800, 17849,   442,  4131,  1068,   287,   764,  1828,\n",
      "          5882, 19685,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22993,  2741,   485,   318,   257,  7404,   287,   262, 16940, 10932,\n",
      "          4783,   286,  2258, 31157,    11,  4492,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [35170,  3961,   645,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 30607,   400,  4687, 14620, 27242,   318,   281, 28621,  1578,\n",
      "          1829,  3701,  5221,  4326,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  7703,  8757,   286, 16070,   318,   257, 31953,  1605, 10574,\n",
      "          1790, 10512,  4635,   416,   262, 17924,    71,   516,   263,  5834,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1212,   318,   262, 33160,  4351,  4324,   329, 17036,  4346,  1622,\n",
      "         33160,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [28718,   324,  2518,    64,  4383,   321,   296,   283,   270,  8083,\n",
      "           318,   257, 44400,   287,   262,  1641, 23509, 31718,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [13787,   379, 22792,   338,   318,   262,  2368,  2107,  5062,   422,\n",
      "          3517, 14015,    12, 34050, 16002,  4186,  1982,    49,  3609,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [42528, 14715, 32294,  8800,  7499,   318,   257,  4429,   319,  6910,\n",
      "           352,   286,   262, 24300, 38536, 12477,   287,  2807,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 6849,    13,   785,   373,   257,  3052,  6898,   416,  2297, 41673,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [32708,  5404,   373,   257,  8300,   983, 15646,   287,  2932, 20510,\n",
      "           416, 12622,   317,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   49,  2373,   495,   318,   257,  8235,  5337,   416,  3271,   311,\n",
      "           418,    77, 12079,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1722,  2645, 44705,   318,   257,   937, 17183,   291, 44400, 34306,\n",
      "           286,   262,  1641,   412,   260, 14065,  3609,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[24761,  5833,    64,  ...,    64, 31329,    11],\n",
      "        [  464,  2975, 23687,  ..., 50256, 50256, 50256],\n",
      "        [ 5219,   286,  7055,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [   53, 33968,  1747,  ..., 50256, 50256, 50256],\n",
      "        [  383, 43247,  1621,  ...,  6570, 30665,    11],\n",
      "        [20459,   691,  4693,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[13924,   390,  8591,  ..., 50256, 50256, 50256],\n",
      "        [   34,   666,   679,  ..., 50256, 50256, 50256],\n",
      "        [   39,   368,   415,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [    1,    43,     6,  ..., 50256, 50256, 50256],\n",
      "        [   44, 23026,   318,  ..., 50256, 50256, 50256],\n",
      "        [    1, 37798,   286,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[ 1238,  4761,   285,  ...,  8207,  3099,   304],\n",
      "        [ 1544,  5341,   329,  ...,  5403,   492, 50256],\n",
      "        [ 6653,   717,  5062,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  464, 21247,   318,  ..., 50256, 50256, 50256],\n",
      "        [  632,   318,   287,  ...,    13, 50256, 50256],\n",
      "        [ 1026,  3568,   355,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[  464, 36999, 16988,  2097,   318,   257,  9566,  2156,   287, 11960,\n",
      "           265,  6415,    11,  8919,    11,  1578,  1829,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [18690,  9802,  2097,  3941,  1467,  4524,   357, 14508,  1900,   355,\n",
      "         25997,  1433,  4524,     8,   318,   257,  3804, 33448, 10828,  2855,\n",
      "           287,   262,   471,    13,    50,    13, 50256, 50256, 50256],\n",
      "        [20644,  1589,   318,   281,  1605, 10359,    12,  8548,  1448,  9393,\n",
      "           287,  9656,   416,   412, 12582,    12,    36,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 5840, 38442, 11356,   357,  6286,  1679,  3945, 11104,     8,   318,\n",
      "           281, 21579,  7246,  1067,   624,  2357,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   53,   562,   318,   257,  3240,   287,  8877,  3418,    11,  2258,\n",
      "          5913,    11,  1578,  1829,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [12211, 13509,   318,   257,  4269,   287,   262,   471,    13,    50,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   42, 10745,  1942,    82, 11312,   318,   257,   678,   400,    12,\n",
      "         14792, 16669,   287,   262, 11905,  7404,   286,   509, 10745,  1942,\n",
      "            82,    11, 29913,   290, 16645,  1214,    13, 50256, 50256],\n",
      "        [   51,   354, 10486,   270,  2852,   292,  3530,   357,  1267,   318,\n",
      "           257,  4675,   287,   968, 12255,    11, 13340,    11,  1578,  1829,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  4343,  4946,   390,   337,   577,    75,   293,   373,   257,\n",
      "          1450,   338, 20790,  7756,  2826,   319, 22639,  1327,  8028,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1925,   392,  5362,   311, 15977,  9869,   272,   318,   281,  3942,\n",
      "         14549,   508,  3568,   287,  3942,  7328,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22253,  6176,   318,   281,  5062,   416,   791, 19103, 45924,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [40002,  3206,  9172,  2753,   867,  1180,  5107,    11,  1390,  1626,\n",
      "           262,   976,  4693,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  271,   257,  4960, 15911,  6802,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 6090,   536,    78,  2059,  7499,   318,   257,  1171,  2267,  6403,\n",
      "           287,  1680,   536,    78,    11, 10836,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  271,   257,  4960, 15911,  3194,   290, 18542,   416, 15804,  5488,\n",
      "           575,   363, 22200,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   35, 43593,   318,   257,  3240,   290,  4783,   286,   262,   509,\n",
      "           459, 16487,    84, 22783,   287,   262,  2619,  6896,  3814,   286,\n",
      "          7137,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [41030,   333,  5049,   318,   257, 27264,  5140,   287,   262,  8473,\n",
      "           286, 36026,    11,  5833,   576,    12, 14772,  1869, 11693,    11,\n",
      "          8602,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   42,  7780,  1228, 11033,    81,  8903,   357, 43819, 15368,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [18673,   541, 21679,   357,  6286, 24648,   287, 36889,   634,     8,\n",
      "           318,   257,   968,  8936, 42844,   272, 24233,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1639,  6387,    89,  6889,  4403,  1400,  1096,   318,   262,  1440,\n",
      "         20283,   290,  2457,  8034,  5062,   416,   262,  3517,  3881,  1448,\n",
      "          3454,   671,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2348,   746,  2591,   376,  1557,   316,   357,    23,  3035,  1248,\n",
      "          3901,   784,  1478,  2795, 32811,     8,   373,   257,  4141, 14971,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  1281,    12, 18302,  9147,   286,  4502,  2669,  2540,   319,\n",
      "          2805,   604,    11,  1596,  5607,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   50,   504,   283,  7499,   318,   257, 12923,  3942, 43443,    12,\n",
      "         16129, 10512,  2646,    11,  7924,   416,   309,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   57,   417,   268,   452,  4914,   357,    11,  1267,   318,   281,\n",
      "          7876,    12,  4906,  9443,   287,   509,   372,  1559,  7567,   295,\n",
      "           286,   509,   372,  1559,  1835, 12957,   287,  7049,    13],\n",
      "        [ 1925,    64,  6210,   318,   281, 15108,  3783,  8842,  5581,  2168,\n",
      "          4635,   416,   604, 40229, 11058,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  5838,   286,   262,  3878, 40515,  5091,   287,   718,  6469,\n",
      "         11843,  1141,   262,  5498, 10626,   268,   666,  1810,   357, 35978,\n",
      "          1906, 35809, 11843,   737, 50256, 50256, 50256, 50256, 50256],\n",
      "        [31633,  4593,  2101,   274,  5513,  6571,  3900,    72,   318,   257,\n",
      "         44400,   286,   262,  1641,   412,   260, 14065,  3609,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [25596,   271, 24017, 21872, 27549,    72, 42445, 21107,   357,  6286,\n",
      "          3426,   838,    11, 24217,     8,   318,   281,  1605, 14971,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [44974, 10644,    82,   318,   257,   471,    13,    50,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [15854,  2189,    64,   318,   281,  5062,   416, 22158, 14015, 28437,\n",
      "           367,  2100,    11,   262,   717,  2716,   739,   607,   898,  1438,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [14295,   311,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  4373, 15682,  1450,   338, 11783,  1074,  6870,  4373,  2059,\n",
      "           287,  1450,   338,  7458,   314, 15244, 24174,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[28477,   287, 35145,  ...,   329,  1719,  1957],\n",
      "        [ 5219,   286, 10433,  ...,   373,  5495,   287],\n",
      "        [  464,  1448,  6198,  ...,  4598,   828, 34179],\n",
      "        ...,\n",
      "        [ 1375,  4271,  2716,  ...,  9126,  1315, 23999],\n",
      "        [24428,  8506,   318,  ..., 50256, 50256, 50256],\n",
      "        [ 2990,  9320,   287,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[   44,   377,   500,  ..., 50256, 50256, 50256],\n",
      "        [    1, 24361,  4934,  ..., 50256, 50256, 50256],\n",
      "        [   33,   445,    82,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  464,  3687,  7152,  ..., 50256, 50256, 50256],\n",
      "        [   32,  8539,   276,  ..., 50256, 50256, 50256],\n",
      "        [   42, 48863,   328,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[ 1026,   318,  5884,  ..., 50256, 50256, 50256],\n",
      "        [  464, 23796,   373,  ...,   287,   262,  2739],\n",
      "        [ 6385, 16489,    11,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  383,   416,    12,  ..., 50256, 50256, 50256],\n",
      "        [44974, 44528,   286,  ..., 38419, 11858,   290],\n",
      "        [  464, 12407,  2156,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[14573, 28328,   373,   257, 12748,    12, 36612,  3098,    12,  2777,\n",
      "           321,  2891,  4635,   416,  4518,  4765,  3457,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   54, 11571,    44,   357, 15377,    13,    18, 18695,     8,   318,\n",
      "           257,  5243,  4429, 22978,   257,  1499,  2647,  5794,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2949,   417, 10805,   357,  6286, 10443, 33905, 10805,    26,  2534,\n",
      "          3426, 37166,   851,  2681,  2932, 15231,     8,   373,   257, 22945,\n",
      "          1067,   624,  2357,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  1583,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 8642, 10602,   357, 31186,  8918, 12768,    12,  1073,     8,   318,\n",
      "           281,  1605,  5156,  3186,  1664,  1912,   287,  3334,  6252,    11,\n",
      "          2258,  5913,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   41,   544, 26421,   357,    67,   798, 34465,   828, 12537,  1438,\n",
      "           575,   392,    84,    11,   373,   257,  3999,  2422,  2276,   286,\n",
      "           262, 17297, 30968,   357, 25540,  1906, 27211,   737, 50256, 50256],\n",
      "        [ 2025,  4712,  9375,   403,   776,   346,  8130,   357,  6286,  2242,\n",
      "          3269, 22717,    25,  1267,   318,   257, 20872, 44824,   272,  2646,\n",
      "         14549,   290, 32010, 26960,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   40,   430,  2735, 21141,   318,   257, 16413,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [21174, 10174,    12, 27869,   357,  6286,  1105,  3267,  9656,     8,\n",
      "           318,   281,  3594,  1067,   624,  2357,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2321,  1906,  1485,  5934,  3448,   313,  3274,  7458,   373,\n",
      "           262,  8915,   400,  1622,   286,   262,  5934,  3448,   313,  1353,\n",
      "            12,  5715,  4346,  4652,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   50,  2645,  8903,   554,  6122,    72,  4849, 34481,   357,    16,\n",
      "          2805, 14062,   784,  2242,  3426,  5816,     8,   373,   257, 26838,\n",
      "         14549,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1925,  4685,   709,   571,  2442,  8583,   318,   257, 34306,   286,\n",
      "          2237,   393,   517, 37115,   393, 10663,    12, 36129,  1512, 38467,\n",
      "           287,   262,  1641,   569,   571,  2442,   325, 48319,    13, 50256],\n",
      "        [29974,   380,   318,   257,  1295,   287,   262,  4783,   286,  6295,\n",
      "           270, 14225,    11,   287,   262,  1181,   286,  2195,   321,    11,\n",
      "          3794,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 9139,  5404,  7212, 29242,    75,   357,  6747,   767,    11, 43785,\n",
      "           784,  3269,  2579,    11, 14745,     8,   373,   257, 33013,   290,\n",
      "         33475,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [35882,  1395,   541,  2634,    71, 10277,   357,  1507,  3459,     8,\n",
      "           318,   257,   645,   303,  8466,   416,   262,  3597, 18545,   449,\n",
      "          7874,    39,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [20508,  1757, 19635,   357,  6286,   860,  3389, 15231,     8,   318,\n",
      "           281,  3594,  1067,   624,  2357,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [32027,   371,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2679,  2344,  5318,  8517,  1345,   321, 27343,  3169,   518,\n",
      "           412, 25649,  2013,  9393,   257, 42119, 16798, 35686,   287,  3648,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [34227,  1754,  3530,   318,   257,   220,  1388,  7627,  1906,  7038,\n",
      "          2975,   287,  8437,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2348,   557,  4496, 22767, 14783,   357,    26,  4642,  2310,  2805,\n",
      "         15963,     8,   318,   281, 10704,  8233,    11, 19834,   290,  1579,\n",
      "         35128,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   43,  2522,   313,   305,  6679, 10872,    64,   357,    11,   366,\n",
      "            66,  2118,    14, 22001,  4695,  4943,   318,   257,   537,   671,\n",
      "           286,  1237,   455,   462,  4695,  1626,   262, 25958,  9752,    13],\n",
      "        [32180, 18816,   992,  5313,   318,   262,  1438,   286,   257,  9513,\n",
      "          5952,   287,  4343,   416,   262,   471,    13,    50,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22405,  6707,   565,   461,   357,  6286,  2932,   604,    11, 21709,\n",
      "             8,   318,   281,  1605, 17646,   290, 18963,  8129,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [23303,   615,   396,    78,   318,   257,  7404,   287,   262,  3240,\n",
      "           286,   509,   668,    74, 10102,    11, 17837,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 11842, 31284,  3418, 25715,   290,  2097,   286, 35074,   318,\n",
      "           262,  1966,  7968,  7356,   329, 11842, 31284,  3418,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [47740, 12139,  8127,   357,  6286,  2805,  2608,    11, 14745,     8,\n",
      "           318,   281,  1605,  1966,  4708,  9283, 43262,   263,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   47,  3974,   282,  4763,   373,   281,  6156,  3942, 36101,   291,\n",
      "         35021,   290, 23723,   287, 16397,  6761,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24119,  1044, 18956,   718,  2624,   357, 12740,   718,  2624,     8,\n",
      "           318,   257,  1181, 12763,   287,   262,   471,    13,    50,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 9360,  4835,   314,   357,    66,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24119,  1044,  3418,   318,   257,  7968,   287,   262, 35618,  6903,\n",
      "           286, 33208,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   46, 21833,   478,   274,  7514,   301,   328, 14664,   318,   257,\n",
      "          4693,   286,  7815,  2376,   620,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [19085,   260,  4244, 18881,  1122,  3961,   318,   257,  2839, 19603,\n",
      "          1524,   329,  1751,   351, 10084,  4673, 12186,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[  392, 12228,   355,  ...,   468,   704,  1296],\n",
      "        [26656, 15385,   284,  ..., 50256, 50256, 50256],\n",
      "        [ 1544,   373,   257,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 3198,   286,  1315,  ..., 50256, 50256, 50256],\n",
      "        [ 1212, 11527, 11084,  ..., 50256, 50256, 50256],\n",
      "        [19085,   260,  4244,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[19156,   347,    13,  ..., 50256, 50256, 50256],\n",
      "        [45670, 12469,   373,  ..., 50256, 50256, 50256],\n",
      "        [22405, 22945,   357,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [43035, 12670, 23955,  ..., 50256, 50256, 50256],\n",
      "        [19156,   327,  3201,  ..., 50256, 50256, 50256],\n",
      "        [ 9360,  6449, 21857,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[15610,  1313,  7504,  ..., 50256, 50256, 50256],\n",
      "        [  464,  1074,   373,  ..., 50256, 50256, 50256],\n",
      "        [ 1558,  1795,   532,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  464,  1438, 20973,  ..., 50256, 50256, 50256],\n",
      "        [ 5219,   286, 10769,  ..., 40394,   663, 11939],\n",
      "        [ 1026,   373,  3417,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[33666, 18011,  5010,   287, 15295, 16707,   389,   257,  1917,   422,\n",
      "           257,  4301,    11,  1919,    11,   290,  3315,  6650,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   44,   378,  1158,   318,   257, 11818, 15201,   666, 11553, 15305,\n",
      "          5140,   287,   943, 46213, 36486,  5665,   286,   262,   943, 46213,\n",
      "         17718,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32, 24503,   516,  4771,   357, 13159,    12, 20470, 32989,   500,\n",
      "           393,   366,    85,   270,   260,   516,     1,  4771,     8,   318,\n",
      "           281,   716, 13425,   516,  4735,  1296,   286,  1660,    13],\n",
      "        [19962,  5030,  7504,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [26886,   406,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  271,   257,  4960, 14971,    11,   262,  8153,   286,   309,  1252,\n",
      "         10145,  3771, 36637,   287,  2869,    11,   717,  7018,   287,  4343,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [25492,   746,  2049, 14685,   373,   257, 14790, 37428,  2049,   326,\n",
      "          7425, 47221,  1141,   262, 23859,  8211, 37428,  2049,  1622,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   35,  3301,   318,   257,   734,    12,  7829, 12531,  4811,  3096,\n",
      "           983,  2826,   287,  1811,  2678,   286,  2688,  5478,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32,  3344,   393,  6274,    64,   576,   373,   257,  7297,   286,\n",
      "           257, 19955,  5428,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [20191,  8616,  6612,   373,   257,  4701,    11,  6853,    11,   290,\n",
      "          1181, 44840,   287,  6835,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [21902,   353,  5567,   318,   257,  2975,   287,   262,  5093,    12,\n",
      "            68,  6470, 21040,   286, 29913,    11,  4885,  4505,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [46686,   286,  1439, 13048,   357, 14508,  1444,  4564,   286,   520,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  3184,  7012,  4479, 13706,   318,   530,   286,  1811, 18836,\n",
      "          9384,   287,   399, 18131,  8482,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   43,   295,   417,   366,    43,   282,     1,   347,   740,   357,\n",
      "           940,  2805,  1248,  2623,   784,   807,  3389, 41507,     8,   373,\n",
      "           257,  3517,  8674,   290, 23139,    13, 50256, 50256, 50256],\n",
      "        [  464,  2864, 26914,  4037,   373,   257,  7756,   319,   262,  2864,\n",
      "         39046,  2159,  9852,   290,  2864,   370,  5603,  9852,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 23327,   317,    13,  1270,   371,    13,    32,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 33195,  1400,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    1,   464,  3776,  1148,  3827,     1,   318,   257,  3496,  6157,\n",
      "           416,  1605,  3881,  4097, 21523,  3798,   594,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22362,   324,   952,  4638, 41826,   623, 12162,    11,   318,   257,\n",
      "         10308,   287, 10062, 12627,    12,    43,   528,   283,   430,    11,\n",
      "         13244,   283,   430,    11,  8602,    13, 50256, 50256, 50256],\n",
      "        [ 2898,   488,   349,  6086,  1126, 35317,  2069,  1460,   318,   281,\n",
      "           556,   283,   291, 39526,   286,   262, 34306,   833,   488,   349,\n",
      "          6086,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32,   907,   529,   283,   310,   544,  8710,   436,    64,   318,\n",
      "           257, 44400,   286,   262,  1641,   412,   260, 14065,  3609,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   36,   929, 31470, 33743,   299,  3692,   346, 18343,   318,   257,\n",
      "         44400,   287,   262,   220,  1641,  2269,   908,  6058,  3609,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [48236,  8315,   318,   262,  8886, 14724,   286,   383, 15551,  4380,\n",
      "            11,  2716,   287, 10249,   416, 23729,  2771, 13407,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   47,  3361,  5733, 10062,   689,   318,   257,  1966, 21649,    12,\n",
      "         26124,   515,  1295,   357,    34,  6322,     8,   287, 19600,   446,\n",
      "          3418,    11,  4744,    11,  1578,  1829,    13, 50256, 50256],\n",
      "        [  198,  1212,   318,   257,  1351,   286,  7328,   351,  1029,  5739,\n",
      "          3965,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [15137,  4198,  5305,   396, 36757,   418,   389,  1900,   284,  2152,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [13217,  3609,   313,  7527,    64, 12376,  3970,   318,   257, 44400,\n",
      "           287,   262,  1641,  2129,   601,  2743, 31718,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10364,   498,  7024,   547,  2714,   287, 37445,   319,  2608,  3945,\n",
      "          1584,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2910, 25996,   357, 10694,    64,   936,  7230,  1045,  1401,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  3648, 49432,  7963, 10749,  1622,   373,   262,   665, 44659,\n",
      "          1622,   286,   262, 49432,  7963, 10749,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [15309, 49078, 30933,   280,  2815,   357,  6286,  1987,  2932, 11104,\n",
      "             8,   318,   257,  4141, 22948,  6441,  2137,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [16108,  3457,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[40764,   287,   262,  ...,  2405,    11,   511],\n",
      "        [ 1026,   318,   530,  ..., 50256, 50256, 50256],\n",
      "        [ 8070,  4771,   318,  ...,  5801, 15134,   286],\n",
      "        ...,\n",
      "        [ 1026,  8096,   257,  ...,   407,  8867,   284],\n",
      "        [ 6653,  2292,   318,  ...,   925,   465,  8886],\n",
      "        [ 1236, 45982,   649,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[14874,   829,  3889,   410,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [15905, 17741,   632,     0,   318,   257, 31349,  4590, 12857,  3586,\n",
      "          2727,   416,  5413,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [40979, 38024, 44272,   370,  3333,   357,  6286,  1315,  3945, 29300,\n",
      "             8,   318,   257, 14780,  3644, 11444,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   56,   263,   324,   318,   257,  7404,   287,   262,  3208,   272,\n",
      "          7193, 14852,    11,  7031,  3301,  5665,   287,   262, 41624,  1181,\n",
      "           286,  3794,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 3163,  3839,   318,   281,  2928, 32593,   287,   262,  9671, 15094,\n",
      "            81,  9248,   319,  8706,   379,   838,    13,  1828,   399,   290,\n",
      "          2808,    13,  6052,  7200,   412,    13, 50256, 50256],\n",
      "        [    1, 11122,   485, 21986,     1,   318,   257,  3496,   416,   262,\n",
      "          3594,  3881,  4097,   262,   569,  3760,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [45534,   710,   282,   318,   281, 16560,  3240,   287, 14327,  3418,\n",
      "            11,  6025,    11,  1578,  1829,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [19184,  3841,   318,   257,  3240,   287, 13643,  3418,    11, 15080,\n",
      "            11,  1578,  1829,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10684,   500,   337,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [34349,   404, 26066,   318,   257, 19342,  1605, 12191,  6754, 10512,\n",
      "          2646,  7924,   416,  7212,   406,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24893, 12236, 13362,  3961,  5665,   318,   257,  1171,  1524,  4783,\n",
      "          1912,   287,  9368, 12236,    11,  3936,   357, 14053,   737, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  818,  1353,  1435,    11,   262,  6383, 35610,  1720,   286,  1353,\n",
      "          2770,  9029,   460,   307,  1813,  1811,  1180,  1353,  5823,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  2864,   513,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   44, 10277,  2001,  5117,   333,  2365, 29603,   220, 29994,   318,\n",
      "           257,  7404,   286,   337,   666,    86,  7344,  5665,   287,   262,\n",
      "         33328,  8473,   286,  7648,    13, 50256, 50256, 50256],\n",
      "        [38708,  5575,    70, 12057,   494,   357,    66,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [37031,   707,   318,   257,  7404,   290,  3026, 28830,   287,   262,\n",
      "          2258, 36889,  4783,   286, 36889,    11,  4492,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   56,  3974,  1453,    12,  2339,   513,   357,    35,  4951,  2522,\n",
      "         10102,     8,   318,   257,  7532,   326,   287,  5384,   318, 30240,\n",
      "           416,   262,   575,    47,  3698,    18,  9779,    13],\n",
      "        [   42,    57,    42,    57,   357, 48555,  1849, 44191,  3001,     8,\n",
      "           373,   262,  1218,  5243,  4429,   287,   262, 13316,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   34,   831, 25116,  6511,   357,    26,  4642,  1679,  3389,  9656,\n",
      "             8,   318,   257,  3999, 20486,   988,    86, 22311, 32110,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [48735,  3147,   322,   501,   220,  7499,   318,   257,  7404,   287,\n",
      "         40753,  2815,   494,    66,  3418,    11,   311,  2915,   666, 20687,\n",
      "           452,  4147,  1056,    11,   287,  8372, 12873,    13],\n",
      "        [35155,   723,   360,  3191,  4912,   357,  8610,    38,   828, 15734,\n",
      "           833, 31111,  4912,    11, 12228,  8646, 10808,   287,   262,  1578,\n",
      "          7526,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [38328,  2059,  3961,   286, 11558,   357,    49,  2937,    44,     8,\n",
      "           318,   257,  2839,  3315,  1524,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [46868, 21835,   357,  1129,  3559,     8,   318,   257,  3517,  3752,\n",
      "            72, 18539,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   35,   488, 12057,   271,  7514,    79, 16260,  1045,   318,   257,\n",
      "         44400,   287,   262,  1641,  2269,   293, 11072, 31718,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  3274, 28370,  9475,   373,   262,  2026,   400, 11957,   286,\n",
      "           262,  5070,   286, 12313,   357, 27429,   737, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   53,  9435,   893,    75,  4170,  9261,   452,  2616,  9300,   591,\n",
      "            72, 12151,   357,    26,  4642,  2808,  1737,  5878,     8,   318,\n",
      "           257, 13053, 18305,  1417,  9422,   647,    13, 50256],\n",
      "        [   32, 11467,   319,   262,  7511, 34732,  3854,   373,  2714,   287,\n",
      "         36498,   319,  1467,  2693,  7358,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  5816,  8533, 16535,  1622,   373,   262,  5846,   400,  1622,\n",
      "           329,   262,  1074,   287,   262,  2351,  9957,  4041,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  271,   257, 15911,  2168,  3194,   416,  4186, 16044,  9075,   461,\n",
      "          6909,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [37776, 18489,   357,  6286,  1367,  3389, 15408,     8,   318,   281,\n",
      "          3594,  4346,  3985,   290,  1966,  4708,  2137,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   47,   417,  1122,   318,   257,  7404,   290, 13901, 15305,   287,\n",
      "          3418, 30928,    11,   287,  4492,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [22327,  5331,  6755,  3839,   318,   281,  4795,  6638,  1492, 12407,\n",
      "          2156,  9393,   287, 14819,   287,  7795,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[40940,   805,    11,  ..., 50256, 50256, 50256],\n",
      "        [15905,  9477,   262,  ..., 50256, 50256, 50256],\n",
      "        [ 1544,   468,  3562,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1544,   318,   262,  ..., 50256, 50256, 50256],\n",
      "        [  464,  3265,   286,  ..., 50256, 50256, 50256],\n",
      "        [20459,  9119,   318,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[    1, 17278,   259,  ..., 50256, 50256, 50256],\n",
      "        [30847, 33533,    64,  ..., 50256, 50256, 50256],\n",
      "        [ 9861,   559,  1073,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [36423,  8299,   364,  ..., 50256, 50256, 50256],\n",
      "        [26754,   318,   281,  ..., 50256, 50256, 50256],\n",
      "        [   37,    17,    72,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[ 1026,   373,  3194,  ...,   374,  1638,   320],\n",
      "        [ 1026,   373,  3417,  ..., 50256, 50256, 50256],\n",
      "        [ 1026,   373,  3417,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1026,   318,   257,  ..., 50256, 50256, 50256],\n",
      "        [26754,   318,  5140,  ..., 50256, 50256, 50256],\n",
      "        [  271,  8031,   338,  ...,  7557,   357, 26391]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[    1,   464,  3776,  1148, 23306,     1,   318,   257,  3496,   416,\n",
      "          3594, 14015,    12, 34050, 16002,  7598,   494,  4243, 11017,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7676, 12898, 32136,   357,  6286,  2932,   807,    11, 25325,     8,\n",
      "           318,   281,  1605,  1772,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 5124, 39193,  6102,  5973,  3900,   357,  6286,  2608,  2932, 28684,\n",
      "             8,   318,   257,  2679,  7500,  3849,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32, 10243, 34099, 16109,   389,   281,  7914, 19907,  3881,  4097,\n",
      "           422, 23995,    11,  8838,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   54,  1506,  9038,  1222, 15215, 19496,   373,   257,  4842,    12,\n",
      "          3106, 27070,  4081,   326,   373,  4075,   422, 41625,   832, 35768,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  1835,   494, 11289,   329,  4307, 46709, 15193,   416,   281,\n",
      "         47144,   373,   717,  5545,   287, 25190,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [18471,  8865,   357,  1778, 17359,   609,   349,     8,   318,   257,\n",
      "          1584,  6085, 10512,  2646,  7924,   416, 16158,  1168,  1069,   263,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [17439, 16131,  4053,   357,  6286,  1467,  2795, 27653,     8,   318,\n",
      "           281,  8685,  3463,    75, 18171,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   41,   461, 34202,  4696,  7499,   318,   257, 15489,  5093,   286,\n",
      "          2258, 49251,  1748,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 24894,   400, 29887,    11, 18671,    37,   373,   257,  4326,\n",
      "           287,   262,  5398, 38076,   560,  5221,  1141,   262,  3274,  2159,\n",
      "          1810,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   50,  2978,   544,   371,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    1, 35792,  7673,   540,   357, 10462,  6457, 12568,    89, 23896,\n",
      "         16725,   318,   257,  3496,   416, 25670, 49595,   330, 35274,   333,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [14573, 19062,   318,   257,  1748,   287,  2629,  9246,   500,   290,\n",
      "          4746, 14683,   287,   262,   471,    13,    50,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  4343,  2258, 21105, 16535,  1622,   373,   262,  1511,   400,\n",
      "           287,   262,  3430,   338,  2106,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   38,  3036, 16706,    11,  5366,  7627,   286,  3932,  9423,  4712,\n",
      "           318,   257, 40738,  7022,   286,   262, 35868,   679,  1671,  1460,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [10910,    12, 10910,   559,   282,   373,   257,   767,   400,    12,\n",
      "         14792, 25798,   286, 15581, 31144,  6264,   287,   662,    12, 36063,\n",
      "           666, 26041,  1035, 24370,    13, 50256, 50256, 50256, 50256],\n",
      "        [  464, 15640,   286,  2275, 10989,   318,   257,  4286,  1492,  3194,\n",
      "           416,  1605,  8204,   263, 42345,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   33,    73, 24172, 35906,  5256,    75,   437,   360, 21241,    71,\n",
      "         14485,   357,  6286,   678,  2795, 15904,     8,   318,   257, 22158,\n",
      "         19834,   290,  9880,  3272,    12, 19315,  1341,   959,    13],\n",
      "        [ 2025,  1605,   833,  1886,    88,   357,  1129,  3132,     8,   318,\n",
      "           257,   662,    12, 10669, 10512,  2646,  7924,   416,  5264,    69,\n",
      "         18042, 22990,  3900,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [12193,  1437,   357,    67,   798, 12279,    18,     8,   373,   257,\n",
      "          7993,  7835, 21772, 28009,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 8205,  6267, 18063,  5015,   220,   318,   262,  1218,  8034,  5062,\n",
      "           416,  1605,  8296,  6802, 20899,   309,  9409,  1000,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   42, 15927,   272,    79, 11033, 11033,   318,   257,  7404,  5140,\n",
      "           287,   262, 27264,   286,   311, 11033,  2584,    75, 11033,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [28711,   317,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [33751, 10602,  1460,   318,   257, 34306,   286, 40941,  1067,   323,\n",
      "         11084, 42560,   284, 46694,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 10443, 24454,  2097,    11,   379,   718,  1731,  8774,   520,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [34220,   400,  4528, 32283,  6736, 27315,   318,   257, 44400,   287,\n",
      "           262,  1641,   327,   859, 14065,  3609,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [36910, 30673,   272,   357,  1129,  1433,  1906, 28296,     8,   373,\n",
      "           257,  3394,    12,  6286, 14780,  8674,   290, 14015,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1212,   318,   257,  1351,   286,  1964,  4671,   287,  7049,    11,\n",
      "          1111,  1613,   290,  1944,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [30098,  6893,  6893,   318,   281, 11553, 15305,   287,   262,   399,\n",
      "          7053,   506, 16522,  5665,   286,   262,   943, 46213, 17718,   286,\n",
      "         37270,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [11922,  1014,   318,   257,  8735,  3942,  2223,  4065, 32251,  7924,\n",
      "           416,  9180,  3301,  1228,   290,  3194,   416,  9910,  1757,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [32027,  6559,  3250,   318,   257,  3952,   287,  5278,  1525,    11,\n",
      "           370,   343,  1373,    11,  4492,    11,  5257,   416,   370,   343,\n",
      "          1373,  4281,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 3041, 13211,   378,   318,   262,  2368,  8034,  5062,   416,  1605,\n",
      "          4334,  6147,  4097, 20843,  1203,   287,  2635,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[ 1026,   373,  2716,  ..., 50256, 50256, 50256],\n",
      "        [ 9360,   717,  5337,  ..., 13661,   286,   262],\n",
      "        [ 1544, 32440,   287,  ...,   492, 50256, 50256],\n",
      "        ...,\n",
      "        [  464,  3807,   373,  ..., 50256, 50256, 50256],\n",
      "        [  464,  9384,   286,  ..., 27702,    13, 50256],\n",
      "        [ 1026,   373,  2716,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[   34, 15918,   399, 11729,   357,    39,   482,    74,  2013,    25,\n",
      "         10884, 26980,   367,  9019,     8,   318,   281,  1248,   400,    12,\n",
      "         14792,  9566,  2615,   287, 49251,    11, 16256,    13],\n",
      "        [   33,   562, 47871,   274,   318,   262,  1438,   286,   257,  4947,\n",
      "           286,  2499,   416, 12702,  1023, 12622,   370,  1025,   268,   290,\n",
      "          6542, 20330,    13, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   43,   363, 38890, 14299,  7499,   318,   257,  7404,   290,   257,\n",
      "          2055,   286,   262,   509,   729,  5362, 27264,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [14731, 23632, 18914,   357,  6286,  3389,  2310,    11, 15524,     8,\n",
      "           318,   257,  1966,  1605,  4346,  6110,   886,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   36,   333,  1045,   965,   328,   452,   298,  2442,   318,   257,\n",
      "         44400,   286,   262,   850, 17989,   943,   310,    72,  1437,    68,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   32,   467, 34619, 45320,  3654,   393,   467, 34619,   318,   257,\n",
      "          3654,   286,   257,  2099,  2968,   287,   262,  1903,  1160,   400,\n",
      "          4289,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  271,   257,  3717,  4960,  6754, 10512,  5581,  2168,    11,   290,\n",
      "           262,  4764,   400, 20486, 13827, 10512,   286, 24451,    42,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7554,  3362,  3873,   323,   357,  6286,  1478,  2795,  9507,   287,\n",
      "         23995,     8,   318,   257, 11905,  1966, 44185,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   47,   325,   463,    84,  7785,   544,  8352, 12204,  5117,    64,\n",
      "           318,   257,  4693,   286,  4618,   287,   262,  1641,  5506,   261,\n",
      "         48319,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 33160,   347,  1601, 21609, 48114,  4281,  3071,   318,  2233,\n",
      "           284,  1011,  1295,   319,   642,  1737, 33160,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   33,   396,  3828,   992,  7499,   318,   257,  7404,   287, 34597,\n",
      "          2879, 27338,  1483,    11, 37875, 23548,  7344, 22783,    11,  8372,\n",
      "            12, 31463, 27902,    13, 50256, 50256, 50256, 50256],\n",
      "        [ 2025, 14172,   299,  3565,   389,   262,  7097,   357,   273,   366,\n",
      "          1676,   525,  4943,  6903,   286,   262,  9686,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7902, 25123,   220,   357, 33481,    70,  3699,    25, 12466,    99,\n",
      "           140,    94,   140,   248,   140,   238,     8,   373,   257, 42119,\n",
      "          5701,  3592,   422, 41427,   544,    11, 27902,    13],\n",
      "        [    1, 26568,  1077,   259,     1,   318,   257,  3496,   416,  1605,\n",
      "         25670,  3998,   262, 16828,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   46, 13087,  1531,   318,   257,  3288,   491,  2676,  3617,  1868,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 3347,   694,    88, 28132,   357,  6286,  8559, 34536,  3469,  3245,\n",
      "            26,  3035,   807,    11, 38525,     8,   318,   281,  1605, 23139,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   46,  2536, 49443,   318,   257, 34306,   286,   262,  3091, 11084,\n",
      "          1641,   440,  2536, 32009, 31718,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    1,  2348,  8664,  1214,     1,   318,   262,  1085,  2060,   290,\n",
      "          3670,  2610,   422,  4403,   370, 11402,   338,  2321,  5062,    11,\n",
      "           978,  8664,  1214,    13, 50256, 50256, 50256, 50256],\n",
      "        [38582,  4586, 10216,   318,   257,   530,    12,   529,   711,   416,\n",
      "         11287,  6484,    11,  3194,   287,   968,  1971,   287, 25177,    13,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24334,   263, 38970,   357,    26,  1267,   318,   257,  3240,   290,\n",
      "          2055,   287, 36538,    11, 11769,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [13217,   359,   328,   672,  3754,   318,   257, 34306,   286,   467,\n",
      "          1525,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  5953,  4139,   286, 33328,    11,   281,  3942,  1181,    11,\n",
      "           318,   262,  1182,   286,   262,  1230,   286, 33328,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [16073, 27593,   357,  6286,  1737,  1315,    11, 15231,     8,   318,\n",
      "           257,  4473,  5052,   319,   262,  1812,  3078,   286,  1024, 41428,\n",
      "            65,  3418,    11,  7859,    13, 50256, 50256, 50256],\n",
      "        [   33,  1638,   377,   746, 20515,  4429,   373,   257, 20515,  4429,\n",
      "          4721,   416,   262,  2258, 30596, 10932, 32130,   287,  1248,  2414,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,   366, 42576,     1,  4676,   318,   262,  2168,  3227,  2746,\n",
      "           286,   257, 14879,  4676,  1444,   402,  9954,  8326,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 6767,   576,  1236,   357,   648,   677,  1143,   355,  1665, 27176,\n",
      "             8,   318,   257, 12822,  2120, 19725,  7404,   287,  3418, 24429,\n",
      "         13528,    11,  7517,    13, 50256, 50256, 50256, 50256],\n",
      "        [   53,   292,   463,  3099,   536,  8590, 48349,   318,   281,   987,\n",
      "         40625,  6802,    11, 10759,  8384,   319, 12036,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [24819, 17417,   286,   921,   318,   262,  5544,  8034,  5062,   416,\n",
      "          1605,  8296,  6802,  8751, 23218,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [21306,   259,    84,  3961,   318,   257,   449,    42,    12,  1065,\n",
      "          1171,  1524,  5140,   287,  6401, 22406,    11, 20572, 42354,    11,\n",
      "          3340,    13, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7554,   317,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   35,   295,   893,   418,   318,   257,  4141,  1461,  4097,  7042,\n",
      "           287,  9656,   287,  3254,   594,    11,  1583, 27083,  1326,    26,\n",
      "           484,  7042,   379,   511, 22404,    66, 22161,    13],\n",
      "        [   46,   647,   418,   318,   281, 12191, 21247,   416,  9281,  7598,\n",
      "           666,  6260, 20893,  6445, 14612,    11,   717,  3199,   287,  6303,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
      "       device='cuda:0', dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[ 1026,   373,  1363,  ...,  2607,   422, 31953],\n",
      "        [  632,   468,   587,  ..., 50256, 50256, 50256],\n",
      "        [ 8421,   262,  2813,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [   44,   888,  2097,  ..., 50256, 50256, 50256],\n",
      "        [ 2990,  1620,  7259,  ..., 50256, 50256, 50256],\n",
      "        [  464,   670,   318,  ...,  2947,   290,   383]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n",
      "{'tokenized_first_sentence': tensor([[14967, 14327,   357,  1129,  2670,    12,  4626,     8,   373,   257,\n",
      "          5650, 11276,   290,  7533,  9991,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 28661, 18660,    37,  6959,    11,   406,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  1853, 19610,  9952,  4041,   318,   262,  5846,   400,  1622,\n",
      "           286,  1353,    12, 24948,  4346,   287, 19610,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  464, 37726,  6484,  7423,  2734, 23931,   373, 12006,   422,  1596,\n",
      "          2154,   284,  1596,  4869,   287,   262, 45262,  3918,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  3738,  8116, 12696,   318,  5140,   287,  9436,  7710,  6233,\n",
      "            11,  9406,    11,  1578,  1829,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [23865, 22586,   397,   357,  6286,  2310,  3945, 15524,     8,   318,\n",
      "           281,  6638,  1602,   395,  4484,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [  464,  3942,  4930, 14187,   786,  7499,   357, 12215,   934,    25,\n",
      "         11243,  9160,   828,   318,   257,  1966, 47491,   286,   262,  3942,\n",
      "         11667, 39463,    13, 50256, 50256],\n",
      "        [ 7676, 28155,   544, 36638,   420, 12204,   271,   318,   257, 44400,\n",
      "           287,   262,  1641,   327,   859, 14065,  3609,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [    1, 36534, 40084,     1,   318,   257,  3496,  6264,   416,  1605,\n",
      "         14015,  1709, 10757,    11,  9593, 25670, 11809,   262,   371, 11463,\n",
      "            13, 50256, 50256, 50256, 50256],\n",
      "        [ 9527,  3609,   271,  7499,   318,   257, 34306,   286, 39513,  7268,\n",
      "           734,  4693,    11,  1444,  3056, 39513,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   32,   354,  8270,   952,  7499,   318,   257,  1966, 27264,   319,\n",
      "           262,  7022,   286,  2744, 20942,    11,   314, 27625, 12010,    11,\n",
      "         10315,    13, 50256, 50256, 50256],\n",
      "        [14993,   451,  1963,   396,   538,  5050,   389,   973,   329,   262,\n",
      "         29052,  4610,   286,  8850, 22577, 27490,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [ 4834,    74, 13415, 33977,   318,   257,  5651,   385, 20515,  4429,\n",
      "           287,  2039,    74, 13415, 33977,    11, 12671,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   38, 12573,   569,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   44,   648,  7087,    64,  1275,   274,   544,   318,   257,  4693,\n",
      "           286, 48573,  4618,   287,   262,  6124,  6391,  1641,    11,  1052,\n",
      "           330, 22490, 48319,    13, 50256],\n",
      "        [30128, 37796,   338,   360,  3191, 10096,   373,   257,  7072, 12228,\n",
      "           503,   286,   262, 11566,   286,  2042, 19500,  4502,   309,    13,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [17121,  8616,  1737,  3245,   357,  6286,  3267,  1596,    11, 25177,\n",
      "             8,   318,   281,  1605,  1966,  4708,  9669,  2137,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   35,    88,   282,   318,   281,   555,  1939, 40132,  2055,   287,\n",
      "         38742,   559,  3418,    11,  4744,    11,  1578,  1829,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [35623, 41505,   357,  6286,  2932,  3261,    11, 14489,     8,   318,\n",
      "           281,  1605,  4708,  4283,  1097, 11717,  4639,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1273,  3216, 14861,  5535,   286, 14044,   290,  8987,   357,    50,\n",
      "          9655,  2767,     8,   373,  4920,   287,  4751,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [28711, 22633,  5714,   357,  6286, 15904,     8,   318,   281,  1605,\n",
      "          1171, 10834,   290,  1772,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [ 7554,   370,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   32, 37194,  3745,   318,   257,  2099,   286, 20509,   973,   287,\n",
      "          2511,   945,  5361,   287, 27000, 19998,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [10842,   265,  1044,  4479,  3334,  3961,   318,  5140,   287, 34744,\n",
      "          1044,    11,  3442,    11,  4916,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [13924,   320, 28749,   318,   257, 21813,  8237,  3814, 19254,   319,\n",
      "           262,  4347,   320, 28749, 27264,   287,   262,   978,  4563,   303,\n",
      "          3814,    13, 50256, 50256, 50256],\n",
      "        [    1,  2437,  1680,   314,  7547,     1,   318,   257,  3496,   416,\n",
      "          1605,  6147,  4097,  5821, 47931,    13, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [12218,  7024,   547,  2714,   287, 47268,   323,   319,  1367,  3945,\n",
      "         15674,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [ 1925,   282, 10215,  2271,   502, 13528,  1952,    64,   318,   257,\n",
      "          4693,   286,  2081, 21264,   287,   262,  1641, 23075, 31718,    11,\n",
      "           262,   366,  7942, 37475,  1911],\n",
      "        [   47,   669,  2164, 20935,  9327,  7499,   318,   257, 20515,  4429,\n",
      "          7351,   350,   669,  2164, 20935,    11, 15238,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [31467,   605,   318,   257,  2107,  5062,   416,  5613,  4345, 17805,\n",
      "            11,  6264,   287,  9768,   290,  2716,   287,  7358,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256],\n",
      "        [   46,  3653,    64,    12,  8940, 23225,   357,    46,  3653,    64,\n",
      "          6964,  1424,     8,   318,   257, 26838,  1877,  1575,  7541,  6333,\n",
      "            13, 50256, 50256, 50256, 50256],\n",
      "        [20795, 48111, 41334,    67,   346,   357,    11,  1267,   318,   257,\n",
      "          1748,   287, 24388,   452,  1835, 12957,   357, 36996,     8,   286,\n",
      "          7049,    13, 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32), 'tokenized_next_50_tokens': tensor([[21111,  7923,   373,  ..., 50256, 50256, 50256],\n",
      "        [    1, 25835,  3972,  ...,   257,  3026,  1175],\n",
      "        [  464,  1622,  2540,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 1026,   318,   257,  ..., 50256, 50256, 50256],\n",
      "        [ 1026,   318,  1900,  ..., 50256, 50256, 50256],\n",
      "        [ 1026,   318, 11032,  ..., 50256, 50256, 50256]], device='cuda:0',\n",
      "       dtype=torch.int32)}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# current problem: 1728/ 30864\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m hypernetwork\u001b[39m.\u001b[39;49mrun_train(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     test_loader\u001b[39m=\u001b[39;49mtest_data_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     stop_editing_index\u001b[39m=\u001b[39;49mstop_editing_index,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     lam\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,  \u001b[39m# 20000\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     lam_testing_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     KL_divergence_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# KL_divergence_loss=False,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     f_data_to_soft_labels\u001b[39m=\u001b[39;49mf_data_to_soft_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "\u001b[1;32m/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb Cell 41\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=455'>456</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpenalty_loss\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=456'>457</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=457'>458</a>\u001b[0m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=458'>459</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters(), max_grad_clip\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=459'>460</a>\u001b[0m )  \u001b[39m# just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=461'>462</a>\u001b[0m \u001b[39m# Check for nan gradients\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=462'>463</a>\u001b[0m \u001b[39m# if check_nan_gradients(self):\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=463'>464</a>\u001b[0m \u001b[39m#     break\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=464'>465</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=465'>466</a>\u001b[0m \u001b[39m# Backwards step\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Ba100-4x/home/sidnbaskaran/hypernetwork-editor/notebooks/editing_wikipedia_sentences.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=466'>467</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/editor/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:20\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_no_grad_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/editor/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:67\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), ([device_grads], _)) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m         (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m _has_foreach_support(device_grads, device))\n\u001b[1;32m     65\u001b[0m         \u001b[39mor\u001b[39;00m (foreach \u001b[39mand\u001b[39;00m _device_has_foreach_support(device))\n\u001b[1;32m     66\u001b[0m     ):\n\u001b[0;32m---> 67\u001b[0m         norms\u001b[39m.\u001b[39mextend(torch\u001b[39m.\u001b[39;49m_foreach_norm(device_grads, norm_type))\n\u001b[1;32m     68\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[1;32m     69\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/editor/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# current problem: 1728/ 30864\n",
    "hypernetwork.run_train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    epochs=200,\n",
    "    lam=0,  # 20000\n",
    "    lam_testing_penalty=0,\n",
    "    KL_divergence_loss=True,\n",
    "    # KL_divergence_loss=False,\n",
    "    f_data_to_soft_labels=f_data_to_soft_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at some outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypernetwork = torch.load(\"checkpoint_epoch_0_batch_8811.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITOR INPUT ID SHAPE torch.Size([1, 28])\n",
      "TARGET HIDDEN SHAPE torch.Size([1, 8, 13, 768])\n",
      "HIDDEN STATE SHAPE torch.Size([1, 28, 768])\n",
      "EDITOR HIDDEN SHAPE torch.Size([1, 28, 768])\n",
      "TARGET HIDDEN SHAPE torch.Size([1, 104, 768])\n",
      "QUERY SHAPE (target activations) torch.Size([1, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([1, 1536])\n",
      "PRJECTED OUTPUT SHAPE torch.Size([1, 104, 768])\n",
      "ATTN OUTPUT SHAPE torch.Size([1, 104, 768])\n",
      "REVERSE ATTENTION OUTPUT SHAPE torch.Size([104, 768])\n",
      "TEMP EDIT VECTORS SHAPE torch.Size([1, 104, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' is not after the ancient English, that the riverccan,\\n bridge was a in a and is back the earlyth century. 18. it is the large II bridge bridge.\\nTheTheTheTheTheTheTheTheTheTheTheThe']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    temp = batch\n",
    "    break\n",
    "hypernetwork.inspect_batch_prediction_ouptuts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITOR INPUT ID SHAPE torch.Size([32, 28])\n",
      "TARGET HIDDEN SHAPE torch.Size([32, 8, 13, 768])\n",
      "HIDDEN STATE SHAPE torch.Size([32, 28, 768])\n",
      "EDITOR HIDDEN SHAPE torch.Size([32, 28, 768])\n",
      "TARGET HIDDEN SHAPE torch.Size([32, 104, 768])\n",
      "QUERY SHAPE (target activations) torch.Size([32, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([32, 1536])\n",
      "PRJECTED OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "ATTN OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "REVERSE ATTENTION OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "TEMP EDIT VECTORS SHAPE torch.Size([32, 104, 768])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader:\n",
    "    result = hypernetwork.forward(\n",
    "        editor_input_ids=batch[\"tokenized_first_sentence\"],\n",
    "        target_input_ids=batch[\"tokenized_next_50_tokens\"],\n",
    "        stop_editing_index=stop_editing_index,\n",
    "        output_target_hidden_states=True,\n",
    "        output_edited_hidden_states=True,\n",
    "        output_edit_vectors=True,\n",
    "        output_editor_attention=True,\n",
    "    )\n",
    "    temp = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITOR INPUT ID SHAPE torch.Size([1, 28])\n",
      "TARGET HIDDEN SHAPE torch.Size([1, 8, 13, 768])\n",
      "HIDDEN STATE SHAPE torch.Size([1, 28, 768])\n",
      "EDITOR HIDDEN SHAPE torch.Size([1, 28, 768])\n",
      "TARGET HIDDEN SHAPE torch.Size([1, 104, 768])\n",
      "QUERY SHAPE (target activations) torch.Size([1, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([1, 1536])\n",
      "PRJECTED OUTPUT SHAPE torch.Size([1, 104, 768])\n",
      "ATTN OUTPUT SHAPE torch.Size([1, 104, 768])\n",
      "REVERSE ATTENTION OUTPUT SHAPE torch.Size([104, 768])\n",
      "TEMP EDIT VECTORS SHAPE torch.Size([1, 104, 768])\n"
     ]
    }
   ],
   "source": [
    "predicted_strings = hypernetwork.inspect_batch_prediction_ouptuts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46172, 21071, 3754, 6711, 357, 66, 13]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "for i in df[\"tokenized_first_sentence\"]:\n",
    "    temp2 = i\n",
    "    y = y + 1\n",
    "    if y == 2000:\n",
    "        break\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDITOR INPUT ID SHAPE torch.Size([32, 28])\n",
      "TARGET HIDDEN SHAPE torch.Size([32, 8, 13, 768])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN STATE SHAPE torch.Size([32, 28, 768])\n",
      "EDITOR HIDDEN SHAPE torch.Size([32, 28, 768])\n",
      "TARGET HIDDEN SHAPE torch.Size([32, 104, 768])\n",
      "QUERY SHAPE (target activations) torch.Size([32, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([32, 1536])\n",
      "PRJECTED OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "ATTN OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "REVERSE ATTENTION OUTPUT SHAPE torch.Size([32, 104, 768])\n",
      "TEMP EDIT VECTORS SHAPE torch.Size([32, 104, 768])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X65sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predicted_strings \u001b[39m=\u001b[39m [tokenizer\u001b[39m.\u001b[39mdecode(pred) \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m predicted_ids]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X65sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Compute the most likely tokens from running gpt2 on the target_input_ids\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X65sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m gpt2_result \u001b[39m=\u001b[39m model(temp[\u001b[39m\"\u001b[39;49m\u001b[39mtokenized_next_50_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mcuda())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X65sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m gpt2_predicted_ids \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39margmax(pred, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m pred \u001b[39min\u001b[39;00m gpt2_result[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/interp/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/interp/lib/python3.10/site-packages/torch/cuda/__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# hypernetwork.evaluate_KL_test_loss_nogradient(data_loader, f_data_to_soft_labels, stop_editing_index=stop_editing_index)\n",
    "# #hypernetwork.inspect_batch_prediction_ouptuts(temp)\n",
    "result = hypernetwork.forward(\n",
    "    editor_input_ids=temp[\"tokenized_first_sentence\"],\n",
    "    target_input_ids=temp[\"tokenized_next_50_tokens\"],\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    output_target_hidden_states=True,\n",
    "    output_edited_hidden_states=True,\n",
    "    output_edit_vectors=True,\n",
    "    output_editor_attention=True,\n",
    ")\n",
    "# compute most likely tokens from the logits\n",
    "predicted_ids = [torch.argmax(pred, dim=-1) for pred in result[\"logits\"]]\n",
    "# convert the token ids to strings\n",
    "predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "\n",
    "# Compute the most likely tokens from running gpt2 on the target_input_ids\n",
    "gpt2_result = model(temp[\"tokenized_next_50_tokens\"].cuda())\n",
    "gpt2_predicted_ids = [torch.argmax(pred, dim=-1) for pred in gpt2_result[\"logits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_first_sentence': tensor([[  464,  6387,   829, 24523,  3834, 21426,   318,   257, 24337,  1627,\n",
       "           6898,   416,  9429,    55, 15198,   287,   262,   471,    13,    50,\n",
       "             13, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  3517,  7963, 10749,   318,   257,  5701,  1097, 11717,  2168,\n",
       "           1912, 20736,   287,   262,  1578,  7526,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 1925, 31131,  8466, 25655, 36749,  7484,   318,   257,  4693,   286,\n",
       "          40941,  4077,  4580,   282,  4908,   287,   262,  7458,   609,  4685,\n",
       "          16982,  8326,    13, 50256, 50256, 50256],\n",
       "         [ 5653, 14620,    11, 11419,  2714,  1877,    12,  6477,  5581, 16625,\n",
       "            290,  5103, 13892,   287,  1811,   471,    13,    50,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [43339, 12398, 38174,   357,  6286,  3945,  2242,    11,  9656,     8,\n",
       "            318,   257,  1966,  4708,  5398,  4346, 18961,    13, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 5990, 12894,   261,   393,  2448, 12894,    78,   357,  2704,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   39,  1211,  1525, 12950,  1322,   318,   257,  3517,  2746, 45247,\n",
       "           9138,  1664,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   45,   533, 24631,  1869, 14403,  7499,   357,  1129,  3365,    30,\n",
       "             12,  1983,  3389,  3717,     8,   373,   257, 15310,  2040,    68,\n",
       "          44185,    13, 50256, 50256, 50256, 50256],\n",
       "         [  464, 23948,   286, 12457,  4951,   544,   318,   257, 10530,  3194,\n",
       "            416, 18623, 39043,   290,  5780,  4243,  4223,   380,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [10082,  2364,  4364,  1982, 15739, 27428,   357,  6286,   352,  3269,\n",
       "          20033,     8,   318,   257,  1751,   338, 37986,   290, 21810,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 12863,    14,  4521,   376,  1797, 38809, 15903,   278,  2159,\n",
       "           5454,   373,   262,   767,   400,  2159,  5454,  1622,   287, 19984,\n",
       "          14284,    13, 50256, 50256, 50256, 50256],\n",
       "         [ 9861,  1734,    64,   299,   425,    64,   318,   257,  4693,   286,\n",
       "          45489,   287,   262,  1641, 17419,   321,  1525,    66, 31718,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 11937,   286,  3942, 16712,   357,    37,  3539,     8,   318,\n",
       "            281, 18091,  2831,  1767,   287,  3794,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   54,   648,   402,  3702,   648,   357,    26,  1737,  2579,    11,\n",
       "          41435,  1849,  1906,  3426,   838,    11,  7795,     8,   373,   257,\n",
       "           3999,  4523, 33013,    13, 50256, 50256],\n",
       "         [10445, 31047,   357, 36234,    11,  2258, 39812,     8,   318,   281,\n",
       "            555,  1939, 40132,  2055,   287, 43178,   292,  3418,    11,  3442,\n",
       "             13, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  5618,  6188, 28403,  8997, 45065, 12713,    12,    18,    11,\n",
       "            357, 11674,  3705,   350, 10526, 12713,    12,    18,   828,   318,\n",
       "            257,   471,    13,    50,    13, 50256],\n",
       "         [ 3791,   367,   622,    64,  1134,  3832,   318,   257,  7404,   287,\n",
       "            262,   609, 28474,  1872,  4783,   286, 29975,   273,   321,    11,\n",
       "           3794,    13, 50256, 50256, 50256, 50256],\n",
       "         [39989, 22864,   357,  6286,  3035,  2608,    11, 28017,     8,   318,\n",
       "            257, 29570,   287, 20383,   313,  1435,   290,   262, 21546, 14733,\n",
       "           3356,    13, 50256, 50256, 50256, 50256],\n",
       "         [32476,   370,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  2681,   400,  3195,    88,  2949,   626,   292, 15434,   318,\n",
       "            281,  8581,   286,  2041, 13304,   284,   262,  1266,   286, 19533,\n",
       "           1515,   292,   290,  3195,  2523,    13],\n",
       "         [   56,   897, 33110,   357,  6286,  2808,  1737,  7358,     8,   318,\n",
       "            257,  5398,  1067,   624,  2357,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  3834, 32679,  9870,   290, 40947,  5018, 11819,   318,   257,\n",
       "           3331,  2615,   379, 48777,   311,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [32434, 15796, 12181,   357,    66,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   47,   292,   270, 21067,   837,   635,  1900,   355,   837,   318,\n",
       "            257, 12175,  9526, 21388, 11210,   286, 22721,    13, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 2202,  2608,  1737,  3050,    11,   379,  1551,  3598,   661,   547,\n",
       "           2923,   287,   257,  5194, 11975,   287,   520,   615,  1773,   349,\n",
       "             11,  3284,    13, 50256, 50256, 50256],\n",
       "         [43568,   367,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  2671,  6005, 40204,   318,   257,  2099,   286, 40204,  6317,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [35649,   265,   623,  3247,  8130,  5665,   318,   257,  4783,   287,\n",
       "            262, 42682,   313,   430,    12,    44,   648, 16522, 17718,   286,\n",
       "          46694,    13, 50256, 50256, 50256, 50256],\n",
       "         [   45,  1077,  4244,   318,   262,  3139,   290,   749, 44042,  1748,\n",
       "            286,   262,   471,    13,    50,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   51,  2049, 33487,   318,   281,   555,  1939, 40132,  2055,   287,\n",
       "          29539,  3418,    11,   287,   262,   471,    13,    50,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 5841,   395,    78, 20000,  5535,   357,    44, 34382,     8,   318,\n",
       "            257,  1171,  2055,  4152,   287, 42810,    78,    11,  3442,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 29290, 12758,   797, 11218,  1400,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'tokenized_next_50_tokens': tensor([[ 9012,   286,  9266,  ..., 50256, 50256, 50256],\n",
       "         [  464,  2168,   373,  ...,   350,   557, 15516],\n",
       "         [ 1026,   468,   257,  ..., 50256, 50256, 50256],\n",
       "         ...,\n",
       "         [ 5219,   286, 11867,  ..., 50256, 50256, 50256],\n",
       "         [ 1026,   318,   636,  ..., 50256, 50256, 50256],\n",
       "         [   16,    11,   393,  ...,  1660,  3608,   415]], device='cuda:0',\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding string:  ['The', ' Boy', 'les', ' Terminal', ' Sub', 'division', ' is', ' a', ' railroad', ' line', ' owned', ' by', ' CS', 'X', ' Transportation', ' in', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', ' the', ' the', ' Alabama', '\\n', ' state', ' from', ' from', ' the', ' Castle', ',', ',', ',', ' to', ',', ',', ',', ' Alabama', ' Alabama', ' of', ' of', ' of', '.', ',', ',', ',', ' to', ' to', ' to', ' to', ' to', ' Castle', ' Castle', ' Castle', ' Castle', ' the', ' the', ' the', ' the', ' the', ' the', ' to', ' to', ' to', ' to', ' to']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' state', ' is', ' from', ' the', ' York', ' to', ' Ala', ',', ' to', ' the', ',', ' Alabama', ',', ' and', ' about', ' total', ' of', ' $', ' miles', '5', ' miles', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['State', ' of', ' Alabama', '.', ' The', ' line', ' runs', ' from', ' New', ' Castle', ',', ' Alabama', ',', ' to', ' Hoover', ',', ' Alabama', ',', ' for', ' a', ' total', ' of', ' 17', '.', '3', ' miles', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' British', ' GT', ' Championship', ' is', ' a', ' sports', ' car', ' racing', ' series', ' based', ' predominantly', ' in', ' the', ' United', ' Kingdom', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' of', ' is', ' created', ' created', ' by', ' the', ' company', ' Racing', ' Racing', ',', ',', ',', ' and', ' for', ' for', ' for', ' for', ' for', ' the', ' the', ' the', ' the', ' the', ',', ' the', ',', ',', ',', ',', ' the', ' the', ' the', ' the', ' by', ' the', ' by', ' by', ',', '.', ',', ',']\n",
      "GPT2 argmax:  ['\\n', ' is', ' originally', ' scheduled', ' by', ' the', ' creators', ' television', ' League', \"'\", ' Association', ' and', ' the', '.', ' has', ' in', ' the', ' first', ' season', ' seasons', ',', ' was', ' produced', ' as', ' the', ' \"', ' Racing', ' Car', ' Series', '.', '\\n', ' series', ' was', ' now', ' in', ' by', ' the', ' British', 'irling', 'ph', 'ane', ' D', 'l', 'le', ',', ' which', ' the', 'ire', 'lli', ' is']\n",
      "Actual string:  ['The', ' series', ' was', ' originally', ' created', ' by', ' the', ' British', ' Racing', ' Drivers', \"'\", ' Club', ' in', ' 1993', ' and', ',', ' for', ' its', ' first', ' two', ' seasons', ',', ' was', ' known', ' as', ' the', ' National', ' Sports', ' GT', ' Challenge', '.', ' The', ' series', ' is', ' currently', ' run', ' by', ' the', ' St', '', 'ph', 'ane', ' Rate', 'l', ' Organisation', ',', ' while', ' P', 'ire', 'lli']\n",
      "\n",
      "\n",
      "Preceding string:  ['Ch', 'lore', 'lla', ' sor', 'okin', 'iana', ' is', ' a', ' species', ' of', ' freshwater', ' green', ' micro', 'al', 'ga', ' in', ' the', ' Division', ' Ch', 'lor', 'ophy', 'ta', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', \"'s\", ' been', ' very', ' of', 'ald', '-', 'green', ' green', ' and', ' color', '.', '.', '.', '.', ' cells', ' to', ' to', ' to', ' to', ' to', ' cells', ' cells', ' cells', ' to', ' to', ' hours', ' to', '\\n', '\\n', ' color', '\\n', 'color', 'color', 'color', 'color', 'color', 'color', 'Color', 'Color', 'Color', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "GPT2 argmax:  [' is', ' been', ' lot', ' of', 'gent', ' green', 'green', ' hue', ',', ' is', ' to', 'y', '.', ' It', ' leaves', ' are', ' into', ' and', ' form', ' a', ' to', ' cells', '.', ' day', ' days', ' 24', ' hours', '.', ' The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' has', ' a', ' characteristic', ' emer', 'ald', '-', 'green', ' color', ' and', ' pleasant', ' grass', ' odor', '.', ' Its', ' cells', ' divide', ' rapidly', ' to', ' produce', ' four', ' new', ' cells', ' every', ' 17', ' to', ' 24', ' hours', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['MS', ' Communications', ',', ' LLC', ' held', ' low', '-', 'power', ' television', ' licenses', ' and', ' construction', ' permits', ' in', ' several', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', ' the', ' the', ' year', 's', '\\n', '\\n', ' the', ' the', ' the', ' the', ' of', ' the', ' the', ' the', '-', ' highest', ' number', ' of', ' of', ' of', ',', ',', ',', ' States', ' States', ',', ' the', ' of', ' of', ' of', ',', ',', ',', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' point', ' point', ' point']\n",
      "GPT2 argmax:  ['.', ' the', ' last', 's', '.', '\\n', '\\xa0', ' the', ' point', ',', 'U', ' was', ' a', ' first', ' largest', 'largest', ' number', ' of', ' subscribers', 'N', 's', ' in', ' the', ' country', ' States', '.', ' with', ' it', ' half', ' of', ' the', ' other', ' were', ' in', 'sold', '.', ' under', '-', ' ', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['states', ' during', ' the', ' 2000', 's', '.', ' ', ' At', ' one', ' point', ' MS', ' Communications', ' held', ' the', ' second', '-', 'highest', ' number', ' of', ' LP', 'TV', ' allocations', ' in', ' the', ' United', ' States', ',', ' but', ' nearly', ' all', ' of', ' the', ' stations', ' remained', ' un', 'built', ' or', ' dark', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Mit', 'chell', ' Barnett', ' (', 'born', ' February', ' 23', ',', ' 1993', ')', ' is', ' a', ' former', ' professional', ' Canadian', ' football', ' linebacker', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' was', ' born', ' by', 'th', ' overall', ' in', ' the', ' 2016', ' Draft', ' Draft', ' Draft', ' the', ',', '-', '-', ' and', ' the', ' the', ' with', ' with', ' the', ' the', ' the', '.', ',', '.', '.', ' seasons', ' seasons', ' seasons', ' and', ' and', ' the', ' and', '-', ' the', ' the', ' and', ' the', ' the', ' the', ' the']\n",
      "GPT2 argmax:  [' is', ' a', ' by', 'th', ' overall', ' by', ' the', ' 2012', ' NFL', ' Draft', ' by', ' the', ' Calgary', ' Tiger', '-', 'C', 'ats', '.', ' was', ' with', ' the', ' Hamilton', ' in', ' July', ' 1', ',', ' 2016', '.', ' He', ' a', ' seasons', ' with', ' two', ' starts', ' with', ' the', ' Bom', '-', 'C', 'ats', ',', ' he', ' was', ' with', ' a', ' free', ' agent', ' with', ' the', ' Toronto']\n",
      "Actual string:  ['He', ' was', ' drafted', ' 59', 'th', ' overall', ' in', ' the', ' 2016', ' CFL', ' Draft', ' by', ' the', ' Hamilton', ' Tiger', '-', 'C', 'ats', ' and', ' signed', ' with', ' the', ' team', ' on', ' May', ' 27', ',', ' 2016', '.', ' After', ' two', ' seasons', ' and', ' 29', ' games', ' with', ' the', ' Tiger', '-', 'C', 'ats', ',', ' he', ' signed', ' as', ' a', ' free', ' agent', ' with', ' the']\n",
      "\n",
      "\n",
      "Preceding string:  ['Per', 'dig', 'on', ' or', ' Per', 'dig', 'o', ' (', 'fl', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' SHARES', ' SHARES', ' SHARES', '\\n', '\\n', '\\n', ' the', ' a', ' the', 'our', 'our', 'our', 'our', 'our', 'ron', 'ron', 'ron', ',', 'an', ',', ',', 'teen', 'an', 'an', 'an', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'ron', 'ron']\n",
      "GPT2 argmax:  ['.', '.', '10', '09', '.', '\\n', ' found', ' major', 'bad', 'our', ' who', ' the', 'ille', '', 'd', ',', ' the', ' late', 'are', 'ral', '', 'an', ' region', ' He', ' years', ' years', ' the', ' companions', ' were', '.', ' including', ' the', ' in', ' of', ',', ' a', ' and', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['119', '0', '', '12', '20', ')', ' was', ' a', ' trou', 'bad', 'our', ' from', ' L', 'esp', '', 'ron', ' in', ' the', ' G', '', 'v', 'aud', 'an', '.', ' Four', 'teen', ' of', ' his', ' works', ' survive', ',', ' including', ' three', ' cans', 'os', ' with', ' melodies', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['H', 'orn', 'by', ' Rail', 'ways', ' is', ' a', ' British', ' model', ' railways', ' manufacturing', ' company', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' are', ' are', ' back', ' to', ' to', ',', ' the', ',', ',', ',', ',', ',', ' the', ',', ' the', ' for', ' the', ' the', 'ano', '.', '.', '.', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for']\n",
      "GPT2 argmax:  [',', ' in', ' back', ' to', ' the', ',', ' the', ',', ' where', ' a', ' of', ' Lloyd', ' was', ' was', ' a', ' letter', ' for', ' a', ' invention', 'yers', 'an', '-', '.', '.', ' The', ' company', ' Me', ' was', ' clock', ' was', ' built', ' in', ' 18', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' roots', ' date', ' back', ' to', ' 1901', ' in', ' Liverpool', ',', ' when', ' founder', ' Frank', ' Horn', 'by', ' received', ' a', ' patent', ' for', ' his', ' Me', 'cc', 'ano', ' construction', ' toy', '.', ' The', ' first', ' clock', 'work', ' train', ' was', ' produced', ' in', ' 1920', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['N', 'are', 'ndra', ' Man', ' Singh', ' ()', ' (', '19', '58', '?', '-', '27', ' November', ' 2009', ')', ' was', ' a', ' Nep', 'ales', 'e', ' footballer', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", ' was', ' first', ' man', 'ali', ' to', ' to', ' win', ' the', ' football', ' in', ',', ',', ',', ' for', ' for', ' for', ' the', ' team', ' team', ' team', ' the', ' Games', ' Games', '.', '.', ' played', '.', ' years', ' years', ' years', ' in', ' for', ' the', ' the', ' the', ',', 'al', ',', 'al', ' and', ' and', ' and', ' and', '.', ' the']\n",
      "GPT2 argmax:  [' is', ' a', ' first', ' person', 'ales', ' to', ' to', ' play', ' for', ' football', ' in', ' the', '.', ' and', ' he', ' for', ' the', ' Indian', ' national', ' team', '.', ' the', ' 2010', ' World', ' Cup', '.', ' He', ' was', ' for', ' every', ' years', ' for', ' the', ' national', ' teams', ' teams', ',', 'ys', 'ra', ' and', ' and', ' and', ' and', ' the', 'an', 'ra', ' Group', ' SC', '\\n', 'The']\n",
      "Actual string:  ['He', ' was', ' the', ' first', ' Nep', 'ali', ' player', ' to', ' play', ' professional', ' football', ' in', ' India', ',', ' and', ' played', ' for', ' the', ' Nepal', ' national', ' team', ' in', ' the', ' 1982', ' Asian', ' Games', '.', ' He', ' played', ' nearly', ' eight', ' years', ' for', ' the', ' two', ' Indian', ' sides', ' M', 'af', 'atl', 'al', ' Group', ' SC', ' and', ' Mah', 'ind', 'ra', ' United', '..', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Fields', ' of', ' Amb', 'ros', 'ia', ' is', ' a', ' musical', ' written', ' by', ' Joel', ' Higgins', ' and', ' Martin', ' Sil', 'vest', 'ri', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' by', ' the', ' UK', ' Park', ' Park', ',', ',', ',', ',', ',', ',', ' in', ' in', ' in', ' was', ' was', ' was', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ',', '-', ' by', ' by', ' by', ' by', ' by', ',', ',', ' by', ',', ' by', ' by']\n",
      "GPT2 argmax:  [' is', ' a', ' by', ' the', ' same', ' Washington', ' area', 'house', ',', ' the', ' York', ',', ' Canada', ' Brunswick', '.', ' the', '.', ' was', ' was', ' performed', ' by', ' John', ' Peck', 'ley', '.', ' who', 'ographed', ' by', ' John', 'ne', ' St', '.', 'Johnson', 'bett', '.', ' and', ' by', ' the', 'st', ' and', ' and', ' performed', ' in', ' by', ' John', ' L', 'inski', '.', '\\n', ' play']\n",
      "Actual string:  ['It', ' was', ' performed', ' in', ' the', ' George', ' Street', ' Play', 'house', ' in', ' New', ' Brunswick', ',', ' New', ' Jersey', ' in', ' 1993', ' and', ' it', ' was', ' directed', ' by', ' Gregory', ' Hur', 'st', ',', ' chore', 'ographed', ' by', ' Lyn', 'ne', ' Taylor', '-', 'Cor', 'bett', ',', ' staged', ' by', ' Hur', 'st', ',', ' and', ' set', ' design', ' by', ' Deborah', ' Jas', 'ien', '.', ' The']\n",
      "\n",
      "\n",
      "Preceding string:  ['Ge', 'off', 'rey', ' Mc', 'Sk', 'imming', ' (', 'born', ' 1', ' January', ' 1962', ')', ' is', ' a', ' children', \"'s\", ' novelist', ' and', ' poet', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' first', ' of', ' the', ' \"', 'th', ' book', ',', ' and', 'icles', ' and', ' and', ' and', ',', ',', 'aunts', ' and', ' the', ' the', ' the', ' the', ' and', ' of', ' of', ' of', '.', '.', '.', '.', '.', ' is', ' is', ' the', ',', ',', 's', '\\n', '\\n', ' \"', ' \"', ' book', ' book', ' book']\n",
      "GPT2 argmax:  [' is', ' a', ' only', ' of', ' the', ' book', 'th', ' series', ':', 'bo', 'icle', ' the', ' the', 'ining', 'lyn', \"'s\", 'born', \"'s\", 'im', ' through', ' travels', ' author', '.', 'is', ' Schl', ' of', '.', ' books', ' novels', '.', '\\n', ' is', ' written', ' written', ' the', ' books', ' of', ' the', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['He', ' is', ' the', ' author', ' of', ' the', ' 20', ' volume', ' Cairo', ' Jim', ' chron', 'icles', ' and', ' Jo', 'ce', 'lyn', ' Os', 'good', ' j', 'aunts', ' and', ' the', ' Ph', 'yll', 'is', ' Wong', ' series', ' of', ' mystery', ' novels', '.', ' He', ' has', ' also', ' published', ' three', ' volumes', ' of', ' poetry', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' 1985', '/', '86', ' F', 'IS', ' Ski', ' Jump', 'ing', ' World', ' Cup', ' was', ' the', ' 7', 'th', ' World', ' Cup', ' season', ' in', ' ski', ' jumping', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", \"'s\", ' the', ',', ',', ',', ' on', ' the', ' March', ' and', ' in', ' in', ' in', ',', 'ica', ',', ',', ',', '.', '.', '.', '.', '.', ' World', ' World', ' World', ' by', ' by', ' by', ' by', '', '', '', '', '', '', '.', ' by', ',', ',', '\\n', ' Wednesday', ' Wednesday', ' Wednesday', ' Wednesday', ' Wednesday']\n",
      "GPT2 argmax:  [' is', ' with', ' the', ' Bay', ',', ' Ontario', ',', ' July', ' July', ',', '.', ' continued', ' in', ' the', 'o', ',', ' New', ' on', ' 8', ' December', ' 1986', '.', '\\n', ' first', ' was', ' Cup', ' winners', ' held', ' by', ' the', 'ias', ',', 'gaard', '', 'nen', ',', ' the', ' Cup', ' by', ' J', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' began', ' in', ' Thunder', ' Bay', ',', ' Canada', ' on', ' 7', ' December', ' 1985', ' and', ' finished', ' in', ' Plan', 'ica', ',', ' Yugoslavia', ' on', ' 23', ' March', ' 1986', '.', ' The', ' individual', ' World', ' Cup', ' was', ' won', ' by', ' Matt', 'i', ' Ny', 'k', '', 'nen', ' and', ' Nations', ' Cup', ' by', ' Austria', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Gl', 'ene', 'a', ' n', 'ive', 'a', ' is', ' a', ' species', ' of', ' beetle', ' in', ' the', ' family', ' Cer', 'am', 'by', 'c', 'idae', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' as', ' the', '.', 'se', 'ma', ' as', ' as', ':', ',', ' It', ':', ':', ':', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', '\\n', '\\n', '\\n', '\\n', '\\n', ' was', ' a', ' in', \"'s\", ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of']\n",
      "GPT2 argmax:  [' is', ' a', ' as', ' the', '.', 'ika', 'v', ' as', ' a', '48', ' as', '\\n', ' was', ' said', ' as', ' the', ' as', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' was', ' described', ' by', ' R', 'it', 'se', 'ma', ' in', ' 18', '92', '.', ' It', ' is', ' known', ' from', ' Indonesia', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Federation', ' of', ' Indian', ' Airlines', ' (', 'F', 'IA', ')', ' is', ' an', ' airline', ' industry', ' body', ' in', ' India', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' are', ' are', ' a', 'i', 'i', ',', ',', ' and', ' and', ' and', ' and', '\\n', '\\n', '\\n', '\\xa0', ' of', ' of', ' by', ' are', ' by', ' by', ' by', ' by', ' by', ' of', ' of', ' of', ' of', ' by', ' are', ' are', ' are', ' the', '\\n', '\\n', '\\xa0', '\\xa0', '\\n', '\\n', '\\n', '\\n']\n",
      "GPT2 argmax:  [',', ' are', ' also', 'epend', 'Go', ',', ' the', ',', ',', ' Jet', 'Air', '.', '\\n', '\\xa0', ' company', ' of', ' the', ' companies', ' are', ' to', ' out', ' by', ' the', ' FIA', ' Board', ' of', ' of', ' the', ' FIA', ' of', ' the', ' of', ' the', ' three', ' associations', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' members', ' are', ' Ind', 'i', 'Go', ',', ' Spice', 'Jet', ' and', ' Go', 'Air', '.', ' ', ' The', ' functions', ' of', ' the', ' FIA', ' are', ' carried', ' out', ' by', ' an', ' Executive', ' Council', ' composed', ' of', ' the', ' heads', ' of', ' each', ' of', ' the', ' member', ' airlines', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['W', 'ang', ' G', 'anch', 'ang', ' (', ';', ' May', ' 28', ',', ' 1907', '\\xa0', '', ' December', ' 10', ',', ' 1998', ')', ' was', ' a', ' Chinese', ' nuclear', ' physicist', '.', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', \"'s\", ' a', ' of', ' the', ' most', ' fathers', ' of', ' the', ' the', ' physics', ',', ',', ' rays', ' and', ' particles', ' particle', '.', '.', ' was', ' was', ' a', ' the', ' the', ' the', ' the', ' of', ' of', ' of', ' of', ' experiments', ' experiments', ' experiments', '-', '-', '-', '-', '-', ' electromagnetic', ',', ',', ' nuclear', ' nuclear', ',', ' nuclear', '-', '-', ' nuclear', ' nuclear']\n",
      "GPT2 argmax:  [' is', ' a', ' of', ' the', ' first', ' members', ' of', ' the', ' culture', ' power', ',', ' and', ' rays', ',', ' the', ' physics', '.', ' He', ' was', ' also', ' a', ' member', ' in', ' the', ' development', ' of', ' physics', 'ation', ' and', ' and', ' and', ' and', '-', 'gravity', 'rom', 'agnetic', ' and', ' experiments', ' and', ' and', ' physics', ' physics', ' and', ' and', '-', 'nuclear', ' weapons', ',', ',', ' and']\n",
      "Actual string:  ['He', ' was', ' one', ' of', ' the', ' founding', ' fathers', ' of', ' Chinese', ' nuclear', ' physics', ',', ' cosmic', ' rays', ' and', ' particle', ' physics', '.', ' Wang', ' was', ' also', ' a', ' leader', ' in', ' the', ' fields', ' of', ' deton', 'ation', ' physics', ' experiments', ',', ' anti', '-', 'elect', 'rom', 'agnetic', ' pulse', ' technology', ',', ' nuclear', ' explosion', ' detection', ',', ' anti', '-', 'nuclear', ' radiation', ' technology', ',']\n",
      "\n",
      "\n",
      "Preceding string:  ['Sen', 'eca', ' (', 'formerly', ',', ' North', ' Fork', ')', ' is', ' an', ' un', 'inc', 'orporated', ' community', ' in', ' Plum', 'as', ' County', ',', ' California', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', \"'s\", ' in', ' the', ' intersection', ' of', ',', ' feet', ' feet', ' feet', '.', '.', '.', '.', '.', ' is', ' is', ' the', ' the', ' the', ' the', ' the', ' River', ' River', ' of', ' of', ' on', ',', '\\n', '\\n', '\\n', '1', '1', 'feet', 'feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet']\n",
      "GPT2 argmax:  [' is', ' in', ' the', ' intersection', ' of', ' about', ',', ' feet', '.', '1', '4', ' meters', ')', ' The', 'eca', \"'s\", ' the', ' at', ' the', ' south', ' side', ' of', ' River', ',', ' about', '\\xa0', ' of', ' the', ' Creek', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' lies', ' at', ' an', ' elevation', ' of', ' 36', '25', ' feet', ' (', '110', '5', ' m', ').', ' Sen', 'eca', ' is', ' located', ' on', ' the', ' North', ' Fork', ' Feather', ' River', ',', ' ', ' north', ' of', ' Twain', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Federal', ' Information', ' Processing', ' Standard', ' Publication', ' 140', '-', '3', ',', ' (', 'FI', 'PS', ' P', 'UB', ' 140', '-', '3', '),', ' is', ' a', ' U', '.', 'S', '.', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', '\\n', ' security', ' security', ' is', ' to', ' use', ' security', ' and', '.', '\\n', ' the', ' is', ' Security', ' Requirements', ' for', ' Security', ' Crypt', ' Security', 'ules', '.', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
      "GPT2 argmax:  [',', ' system', ' system', '.', ' by', ' protect', ' the', ' keys', ' for', '\\n', ' standard', ' of', ' a', ' and', ' for', ' the', 'ographic', ' Module', 'ules', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['government', ' computer', ' security', ' standard', ' used', ' to', ' approve', ' cryptographic', ' modules', '.', ' The', ' title', ' is', ' Security', ' Requirements', ' for', ' Crypt', 'ographic', ' Mod', 'ules', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['New', ' H', 'ru', 'a', 'ik', 'awn', ' is', ' a', ' village', ' in', ' the', ' Ch', 'amph', 'ai', ' district', ' of', ' Miz', 'or', 'am', ',', ' India', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", ' a', ' in', ' the', ' heart', 'aw', 'b', 'ung', '.', '.', '.', '.', '.', '.', '.', '.', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'A', 'A', 'The', 'The', 'The', 'The', 'A', 'A', 'A', 'A', 'A']\n",
      "GPT2 argmax:  [' is', ' not', ' in', ' the', ' heart', 'ar', 'aja', ' area', 'a', 'atch', 'A', '.', ' area', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' located', ' in', ' the', ' Kh', 'aw', 'b', 'ung', ' R', '.', 'D', '.', ' Block', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Allen', ' Klein', ' (', 'born', ' April', ' 26', ',', ' 1938', ')', ' is', ' a', ' pioneer', ' in', ' gel', 'ot', 'ology', ' and', ' the', ' therapeutic', ' humor', ' movement', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', ',', ',', ',', ',', \"'s\", ' married', ' married', ' a', ' years', ' when', ' when', ' she', ' of', ' of', ' of', ' of', ',', ',', ' and', ' the', ' her', ' had', ' had', ' had', ' of', ' of', ' of', ' of', ' the', ' to', ' to', ' to', ' the', ' the', ' to', ' to', ' to', ' to', ' to', ' career', ' career', ' career', ' career', ' career', ' the', ' and']\n",
      "GPT2 argmax:  [' the', ',', ' the', ' was', ' book', ',', ' diagnosed', ' a', ' years', ' old', '.', ' she', ' died', '.', ' cancer', ' cancer', '.', ' and', ' she', ' couple', ' of', ' she', ' was', ' been', ' her', ' liver', ' of', ' humor', ' and', ' her', ' way', ' through', ' the', ' end', ' of', ' her', ' to', ' write', ' up', ' her', ' job', ' job', ' as', ' a', ' writer', ' director', ' film', ' director', ' writer', '.']\n",
      "Actual string:  ['In', ' 1974', ',', ' Klein', \"'s\", ' wife', ' was', ' only', ' 34', ' years', ' old', ' when', ' she', ' died', ' of', ' liver', ' disease', ',', ' and', ' the', ' aspect', ' where', ' she', ' had', ' kept', ' her', ' sense', ' of', ' humor', ' all', ' the', ' way', ' to', ' the', ' end', ' inspired', ' Klein', ' to', ' give', ' up', ' his', ' previous', ' career', ' as', ' a', ' theater', ' and', ' television', ' scene', ' designer']\n",
      "\n",
      "\n",
      "Preceding string:  ['Henry', ' W', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', '\\n', '\\xa0', '\\xa0', ',', ',', ')', ' ', ' ', ')', ',', ',', ')', ' a', ',', ',', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was']\n",
      "GPT2 argmax:  [',', ',', '\\xa0', '1', ' 2010', ',', ' 2012', ')', ' December', ' 8', ',', ' 18', ')', ' a', ' a', ' member', ' wrestler', ' player', ' who', ' played', ' for', ' base', ' for', ' the', ' New', ' League', 'agues', '.', ' the', ' New', '76', ' and', '.', ' Louis', ' Cardinals', '.', ' He', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Full', 'er', ' ', ' (', 'December', ' 5', ',', ' 1862', ' ', ' December', ' 12', ',', ' 1895', '),', ' was', ' a', ' professional', ' baseball', ' player', ' who', ' played', ' third', ' base', ' in', ' the', ' Major', ' Le', 'agues', ' for', ' the', ' 18', '91', ' St', '.', ' Louis', ' Browns', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' 27', 'th', ' TV', 'y', 'No', 'vel', 'as', ' Awards', ' is', ' an', ' Academy', ' of', ' special', ' awards', ' to', ' the', ' best', ' of', ' soap', ' oper', 'as', ' and', ' TV', ' shows', '.']\n",
      "Predicted argmax:  [' first', ' show', ' was', ' place', ' on', ' the', ' 11', ',', ' 2017', ' at', ' the', ' city', ' of', 'o', ' in', ' Palace', ' the', 'ap', 'ul', 'co', ',', ' Mexico', ',', ' The', ' ceremony', ' was', ' attended', ' on', ' Spanish', ' country', ' City', ' Univ', '+', ' la', ' Cas', 'as', 'as', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "GPT2 argmax:  ['\\n', ' show', ' was', ' place', ' at', ' the', ' 11', ',', ' 2017', '.', ' New', ' city', ' of', 'o', ',', ' Palace', ' a', 'ap', 'ul', 'co', ',', ' Mexico', ',', '\\n', ' ceremony', ' was', ' attended', ' on', ' Spanish', ' United', ' City', ' Univ', ' Plus', ' la', ' Cas', 'as', 'as', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['The', ' awards', ' ceremony', ' took', ' place', ' on', ' March', ' 15', ',', ' 2009', ' in', ' the', ' Forum', ' Mund', 'o', ' Imperial', ',', ' Ac', 'ap', 'ul', 'co', ',', ' Guerrero', '.', ' The', ' ceremony', ' was', ' televised', ' in', ' the', ' Mexico', ' by', ' Canal', ' de', ' las', ' est', 'rell', 'as', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Y', 'ax', ' Patel', ' (', 'born', ' 29', ' May', ' 1999', ')', ' is', ' a', ' Canadian', ' cr', 'ick', 'eter', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' the', ' 2015', ',', ',', ' was', ' diagnosed', ' the', ' the', ' the', ' for', ' the', ' for', ' the', '', '20', '50', ' the', ' World', ' the', '.', '.', '.', '.', '.', '.', '.', '.', ',', '.', ' on', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 argmax:  [' the', ',', ',', ' the', ' will', ' named', ' the', ' the', \"'s\", ' top', ' for', ' the', ' World', ' World', '20', ' season', ' Championship', ' Rugby', '.', '.', ' the', ' United', '.', '.', '\\n', ' was', ' his', ' international', ' of', ' debut', ' in', ' the', ' May', ',', ',', ' and', ' the', \"'s\", ' South', ' United', 'in', 'ard', ' Islands', '.', ' and', ' the', ' Caribbean', ' Super', '50', ' tournament']\n",
      "Actual string:  ['In', ' October', ' 2019', ',', ' he', ' was', ' named', ' in', ' Canada', \"'s\", ' squad', ' for', ' the', ' 2019', '', '20', ' Regional', ' Super', '50', ' tournament', ' in', ' the', ' West', ' Indies', '.', ' He', ' made', ' his', ' List', ' A', ' debut', ' on', ' 8', ' November', ' 2019', ',', ' for', ' Canada', ' against', ' the', ' Le', 'ew', 'ard', ' Islands', ',', ' in', ' the', ' Regional', ' Super', '50']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Sub', 'urban', ' Trust', ' and', ' Savings', ' Bank', ' Building', ' is', ' a', ' bank', ' building', ' at', ' 840', ' S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' is', ',', ',', ',', ' Park', ' Park', ',', '\\n', '\\n', ' was', ' was', ' built', ' is', ',', ' the', ' the', ' the', ' of', ' of', ' the', ' of', ' building', ' building', ' building', ' building', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' with', ' with', ' with', ' with', ' is', ' is', ' is', ' is']\n",
      "GPT2 argmax:  [',', ',', ',', ' and', ' Park', ' Avenue', ' IL', '.', '\\n', ' is', ' the', ' in', ' 18', ' phases', ',', ' one', ' the', ' first', ' being', ' being', ' the', ' building', ' being', ' completed', ' in', ' 18', '.', ' the', ' in', ' May', ' 1', ',', ' 1926', '.', ' The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Oak', ' Park', ' Avenue', ',', ' Oak', ' Park', ',', ' Illinois', '.', ' It', ' was', ' built', ' in', ' two', ' stages', ',', ' with', ' the', ' first', ' portion', ' of', ' the', ' building', ' being', ' built', ' in', ' 1925', ' and', ' opening', ' on', ' May', ' 1', ',', ' 1926', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Howard', ' Douglas', ' Grant', ' (', 'c', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', '\\n', '\\n', '\\n', ',', ',', ',', '\\n', '\\n', '\\n', '\\n', ' the', '\\n', 'bred', 'bred', '.', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ' the', ' the', ' and', ' the']\n",
      "GPT2 argmax:  ['.', ',', ' 18', ' 18', ',', ' 18', '\\n', '\\n', ' a', ' American', '-', 'orough', 'bred', ' racing', ' racing', ' series', 'ockey', '.', ' He', ' in', ' New', ',', ' Ohio', ',', ' in', ' was', ' racing', ' racing', 'ockey', ' career', 'hip', ' at', ' a', ' young', '-', 'year', '-', 'old', ' in', ' the', 'eling', ',', ' in', ' where', ' Virginia', '.', ' then', ' the', ' first', ' race']\n",
      "Actual string:  ['19', '39', ' ', ' August', ' 1', ',', ' 2018', ')', ' was', ' an', ' American', ' Th', 'orough', 'bred', ' horse', ' racing', ' j', 'ockey', '.', ' Born', ' in', ' Cincinnati', ',', ' Ohio', ',', ' he', ' began', ' his', ' j', 'ockey', ' apprentices', 'hip', ' as', ' a', ' seventeen', '-', 'year', '-', 'old', ' at', ' Whe', 'eling', ' Downs', ',', ' West', ' Virginia', ' and', ' won', ' his', ' first']\n",
      "\n",
      "\n",
      "Preceding string:  ['P', 'as', 'it', 'hee', ',', ' also', ' known', ' as', ',', ' is', ' a', ' retro', 'grade', ' irregular', ' satellite', ' of', ' Jupiter', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' that', ' a', ' group', ' of', ' scientists', ' that', ' that', ' University', ' of', ' of', ' the', ' by', ' by', '.', '.', '.', ',', '.', ',', '.', '.', ' the', ' the', ' the', ' the', ' the', ' to', ' to', ' to', ' to', ' to', ' of', ' of', ' of', ' of', ' of', ' to', ' to', ' to', ' to']\n",
      "GPT2 argmax:  [' is', ' a', ' that', ' a', ' team', ' of', ' researchers', ' who', ' the', ' University', ' of', ' California', ' at', ' by', ' Dr', ' W', '.', ' H', 'ppard', ',', ' the', '.', ' who', ' it', ' the', ' name', ' name', ' of', '...', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' was', ' discovered', ' by', ' a', ' team', ' of', ' astronomers', ' from', ' the', ' University', ' of', ' Hawaii', ' led', ' by', ' Scott', ' S', '.', ' She', 'ppard', ' in', ' 2001', ',', ' and', ' given', ' the', ' temporary', ' designation', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['On', ' 26', ' May', ' 2010', ',', ' at', ' least', ' seven', ' people', ' were', ' killed', ' in', ' a', ' bomb', ' blast', ' in', ' St', 'av', 'rop', 'ol', ',', ' Russia', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', ' the', ' the', ' hours', ' of', ' have', ' in', ' one', ' from', ',', ',', ',', ',', ' is', ' a', ' a', ',', ' the', ',', ' from', ' Turkey', ' Turkey', ' Turkey', '.', '.', ' and', ' was', ' the', ' a', '\\n', ',', '\\n', '\\n', '\\n', '\\n', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'A']\n",
      "GPT2 argmax:  [' the', ' one', ' people', ' were', ' killed', ' in', ' including', ' of', ' the', ' and', ' one', ' a', ' was', ' in', ' American', '.', ' according', ' a', ' is', ' the', '.', ' Azerbaijan', '.', '\\n', ' Russian', ' was', ' in', ' the', ' concert', ' in', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['At', ' least', ' 40', ' people', ' were', ' injured', ',', ' one', ' from', ' Moscow', ',', ' while', ' another', ' is', ' an', ' outsider', ',', ' and', ' another', ' from', ' Azerbaijan', ' or', ' Turkey', '.', ' The', ' blast', ' occurred', ' before', ' a', ' concert', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Elizabeth', ' H', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'A', '\\n', '\\n', '\\n', '\\n', ',', ',', ',', ',', ')', ',', ',', ',', ')', ')', ')', ')', ' a', ' a', ' a', ' and', ' a', ' a', ' in', ' in', ' and', ' and']\n",
      "GPT2 argmax:  ['\\n', ')', '\\n', ' (', 'Howard', ')', ',', ' 2016', '83', ')', ' March', ' 1', ',', ' 18', ')', ' p', ' a', ' member', 'ibrarian', ' at', ' a', 'ivist', ' of', ' in', ' the', ' Library', ' States', ' Library', ' the', ' Civil', ' part', 'th', ' century', '.', ' He', ' was', ' a', ' to', ' first', ' State', ' Library', 'ibrarian', ' in', ' 18', ' and', ' and', ' appointed', ' years', ' Texas', ' of']\n",
      "Actual string:  ['(', 'Howard', ')', ' West', ' (', 'March', ' 23', ',', ' 18', '73', ' ', ' January', ' 4', ',', ' 1948', '),', ' was', ' a', ' l', 'ibrarian', ' and', ' arch', 'ivist', ' active', ' in', ' the', ' United', ' States', ' during', ' the', ' early', ' 20', 'th', ' century', '.', ' West', ' was', ' appointed', ' the', ' Texas', ' State', ' L', 'ibrarian', ' in', ' 1918', ',', ' was', ' two', ' time', ' President']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Gl', 'aser', ' coupling', ' is', ' a', ' type', ' of', ' coupling', ' reaction', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' the', ' the', ' the', ' of', 'yl', 'ic', ',', ' is', ' is', ' on', ' on', 'rous', 'rous', ',', ',', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide']\n",
      "GPT2 argmax:  [' is', ' not', ' no', ' the', ' most', ' known', 'amin', 'ch', ' acid', ' molecule', ' the', ' the', ' on', ' the', 'id', 'al', '.', ' acet', ',', 'II', ')', ' and', ' and', ' copper', '(', 'II', ')', ' chloride', 'rom', 'ide', '.', ' is', 'hyd', ' copper', 'izing', ' called', ' sodium', '.', ' It', ' acet', ' of', ' the', ' base', ' form', ' is', ' a', '.', ' The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' by', ' far', ' the', ' oldest', ' acet', 'yl', 'enic', ' coupling', ' and', ' is', ' based', ' on', ' cup', 'rous', ' salts', ' like', ' copper', '(', 'I', ')', ' chloride', ' or', ' copper', '(', 'I', ')', ' b', 'rom', 'ide', ' and', ' an', ' additional', ' oxid', 'ant', ' like', ' oxygen', '.', ' The', ' base', ' in', ' its', ' original', ' scope', ' is', ' ammonia', '.', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Amb', 'at', 'ond', 'raz', 'aka', ' District', ' is', ' a', ' district', ' in', ' the', ' Ala', 'ot', 'ra', '-', 'M', 'ang', 'oro', ' Region', ' of', ' Madagascar', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', ' a', ' is', ' a', ' city', ' of', ' H', ',', 'ond', 'raz', ',', ',', ',', ' is', ' is', ' of', ' of', ' of', ' of', ' of', ' the', ' population', ' in', ' in', ' was', ',', ',', ',', ',', '\\n', '\\n', 'The', 'The', 'The', 'R', 'R', 'R', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is']\n",
      "GPT2 argmax:  [',', ',', ' in', ' city', ' of', ' K', 'man', ',', ',', ',', ',', ' It', ' town', ' is', ' been', ' estimated', ' of', ' about', ' which', ' is', ' capital', ' population', ' is', ' the', ' was', ' 1', ',', '000', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' capital', ' is', ' the', ' town', ' of', ' Am', 'bat', 'ond', 'raz', 'aka', '.', ' The', ' district', ' has', ' an', ' area', ' of', ',', ' and', ' the', ' estimated', ' population', ' in', ' 2013', ' was', ' 324', ',', '610', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['N', 'ash', 'ville', ' is', ' the', ' capital', ' and', ' most', ' populous', ' city', ' of', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' of', ' the', '\\n', '\\n', \"'s\", ' a', ' of', ' of', ' of', ' of', ' County', ' of', ' of', ' of', ' of', ' County', ' County', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' was', ' a', ' first', ' seat', ' of', ' the', ' County', ',', ' is', ' held', ' in', ' the', ' western', 'land', ' River', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['state', ' of', ' Tennessee', '.', ' It', ' is', ' the', ' county', ' seat', ' of', ' Davidson', ' County', ' and', ' is', ' located', ' on', ' the', ' Cumber', 'land', ' River', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['T', 'oon', 'erville', ' is', ' an', ' un', 'inc', 'orporated', ' community', ' in', ' Pike', ' County', ',', ' in', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', '-', ' the', '\\n', '\\n', \"'s\", \"'s\", ' in', ' the', ' States', ' State', ' the', ',', ',', '\\xa0', '\\xa0', '\\xa0', '.', ',', 'km', 'km', ')', ',', ' of', ' of', ',', ',', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', ' US', ' US', ' US', ' US', ' US', ' US', ' US', ' US']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' is', ' turn', ' in', ' the', ' States', '.', ' Kentucky', ' of', ' miles', ' miles', '\\xa0', 'about', ' about', ' km', ' km', 'mi', ')', ' of', ' of', ' the', ',', ' DC', ' state', \"'s\", ' largest', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['state', ' of', ' Kentucky', '.', ' It', ' in', ' located', ' in', ' United', ' States', ' -', ' some', ' 301', '\\xa0', 'mi', ' (', 'or', ' 48', '5', '\\xa0', 'km', ')', ' West', ' of', ' Washington', ',', ' the', ' country', \"'s\", ' capital', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Mod', 'est', 'o', ' Junior', ' College', ' (', 'M', 'JC', ')', ' is', ' a', ' public', ' community', ' college', ' in', ' Modest', 'o', ',', ' California', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' of', ' the', ' Valley', \"'s\", \"'s\", \"'s\", ' in', ' Columbia', '.', '.', ',', ',', ',', ' Columbia', ' Columbia', ',', ' to', ' to', ' to', ' College', ' College', ' College', ' College', ' with', ' with', ' with', ' community', ' to', ' to', ',', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' to', \"'s\", \"'s\", \"'s\", \"'s\"]\n",
      "GPT2 argmax:  [' is', ' not', ' of', ' the', \"'s\", ' Park', \"'s\", \"'s\", ' with', ' the', ' University', ' and', '\\n', ' is', ' is', ' a', ' the', ' College', ',', ' are', ' to', ' the', ' same', ' State', ' College', ' District', '.', ' with', ' the', ' other', ' community', ' colleges', ' colleges', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' part', ' of', ' Yosemite', ' Community', ' College', ' District', ' along', ' with', ' Columbia', ' College', '.', ' MJ', 'C', ',', ' and', ' Columbia', ' College', ',', ' belong', ' to', ' the', ' California', ' Community', ' College', ' system', ' along', ' with', ' 112', ' other', ' public', ' community', ' colleges', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Whites', 'hell', ' Re', 'actor', ' No', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' SHARES', '.', ' the', '-', '1', '\\n', '\\n', ' a', ' a', ' a', ' at', ' at', ' at', ' at', 'EC', '.', \"'s\", '.', '.', ' (', ' in', ' in', ' in', '.', ' was', ' was', ' was', ' was', ' was', ' was', ' was']\n",
      "GPT2 argmax:  ['.', '000', ' a', '3', '1', ',', ' or', ' the', ' \"', ' national', ' project', '.', ' in', ' the', '.', 'ON', ',', ' A', 'ide', ' facility', ' in', 'W', 'L', ')', ' in', ' the', ',', ' The', ' was', ' the', ' in', ' produce', ' the', ' feasibility', ' of', ' a', ' nuclear', '-', 'U', '-', '2', ' reactor', '.', ' could', ' the', ' existing', '-', ' reactor', 'ant', ' reactor']\n",
      "Actual string:  ['1', ',', ' or', ' WR', '-', '1', ',', ' was', ' a', ' Canadian', ' research', ' reactor', ' located', ' at', ' A', 'EC', 'L', \"'s\", ' Whites', 'hell', ' Laboratories', ' (', 'WN', 'RL', ')', ' in', ' Manitoba', '.', ' It', ' was', ' built', ' to', ' test', ' the', ' concept', ' of', ' a', ' C', 'AND', 'U', '-', 'type', ' reactor', ' that', ' replaced', ' the', ' heavy', ' water', ' cool', 'ant']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare against the actual strings from the batch\n",
    "for i in range(len(predicted_strings)):\n",
    "    print(\n",
    "        \"Preceding string: \",\n",
    "        tokenizer.batch_decode(temp[\"tokenized_first_sentence\"][i]),\n",
    "    )\n",
    "    print(\"Predicted argmax: \", tokenizer.batch_decode(predicted_ids[i]))\n",
    "    print(\"GPT2 argmax: \", tokenizer.batch_decode(gpt2_predicted_ids[i]))\n",
    "    print(\n",
    "        \"Actual string: \", tokenizer.batch_decode(temp[\"tokenized_next_50_tokens\"][i])\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding the first element of batch\n",
    "(\n",
    "    tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]),\n",
    "    tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]),\n",
    "    tokenizer.decode(batch[\"tokenized_first_sentence\"][0][0]),\n",
    "    tokenizer.decode(batch[\"tokenized_first_sentence\"][0][1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output['edited_hidden_states'][0][0,7][0:10] - model(cat_example,output_hidden_states = True).hidden_states[0][0][7][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_embedding[0:10]\n",
    "result[\"editor_attention\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block calculates edit norm relative to the size of the current activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for batch_index in range(min(20, len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "    # The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor[:8, :, :] = edit_tensor[:8, :, :] / result[\"target_hidden_states\"][\n",
    "        batch_index\n",
    "    ].norm(dim=2, keepdim=True).to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm / Target Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_first_sentence\"][batch_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for batch_index in range(min(10, len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "    # The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "stopping_index = 8\n",
    "\n",
    "for head_index in range(min(hypernetwork.editor_model.config.num_editing_heads, 10)):\n",
    "    for batch_index in range(3):\n",
    "        # Reshape the tensor into an 8x13 matrix\n",
    "        attention_matrix = (\n",
    "            result[\"editor_attention\"][batch_index][head_index].reshape(8, 13).to(\"cpu\")\n",
    "        )\n",
    "\n",
    "        # is this any better??\n",
    "        # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "        # Detach and convert to numpy\n",
    "        attention_matrix = attention_matrix.detach().numpy()\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig, ax = plt.subplots()\n",
    "        heatmap = ax.imshow(attention_matrix, cmap=\"hot\")\n",
    "\n",
    "        # Color the heatmap according to the entry sizes\n",
    "        heatmap.set_clim(vmin=np.min(attention_matrix), vmax=np.max(attention_matrix))\n",
    "        cbar = plt.colorbar(heatmap)\n",
    "        cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "        # Add labels to the x and y axes\n",
    "        ax.set_xticks(np.arange(13))\n",
    "        ax.set_yticks(np.arange(8))\n",
    "        ax.set_xticklabels(np.arange(13))\n",
    "        ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "        # Rotate the x-axis labels\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(\"Editor Attention Heatmap\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        print(tokenizer.batch_decode(batch[\"result_text\"][batch_index]))\n",
    "        print(tokenizer.batch_decode(batch[\"editor_tokens\"][batch_index][8:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(\n",
    "#     result[\"edit_vectors\"][batch_index][:stopping_index, :, :].to(\"cpu\"), dim=[0, 2]\n",
    "# )  # looks better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: It seems, currently, like we are not giving sufficient incentive to intervene at the lowest possible layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_tensor = result[\"edit_vectors\"][batch_index].reshape(8, 13, -1).to(\"cpu\")\n",
    "edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "edit_tensor_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_states = torch.stack(result[\"edited_hidden_states\"], dim=2)\n",
    "edited_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(edited_states[0][i][0].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    torch.norm(result[\"target_hidden_states\"][0][6][0]),\n",
    "    torch.norm(result[\"target_hidden_states\"][0][0][0]),\n",
    "    edited_states[0][6][0].norm(),\n",
    "    edited_states[0][2][0].norm(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(hypernetwork.state_dict(), \"/root/aiplay-1/hypernetworks/hypernetwork.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
