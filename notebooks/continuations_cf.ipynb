{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../assets/data/counterfact.json\"\n",
    "# load the json file\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'case_id': 0,\n",
       " 'pararel_idx': 2796,\n",
       " 'requested_rewrite': {'prompt': 'The mother tongue of {} is',\n",
       "  'relation_id': 'P103',\n",
       "  'target_new': {'str': 'English', 'id': 'Q1860'},\n",
       "  'target_true': {'str': 'French', 'id': 'Q150'},\n",
       "  'subject': 'Danielle Darrieux'},\n",
       " 'paraphrase_prompts': ['Shayna does this and Yossel goes still and dies. Danielle Darrieux, a native',\n",
       "  'An album was recorded for Capitol Nashville but never released. Danielle Darrieux spoke the language'],\n",
       " 'neighborhood_prompts': ['The mother tongue of Léon Blum is',\n",
       "  'The native language of Montesquieu is',\n",
       "  'François Bayrou, a native',\n",
       "  'The native language of Raymond Barre is',\n",
       "  'Michel Rocard is a native speaker of',\n",
       "  'Jacques Chaban-Delmas is a native speaker of',\n",
       "  'The native language of François Bayrou is',\n",
       "  'Maurice Genevoix, speaker of',\n",
       "  'The mother tongue of François Bayrou is',\n",
       "  'Melchior de Vogüé, speaker of'],\n",
       " 'attribute_prompts': ['J.\\xa0R.\\xa0R. Tolkien is a native speaker of',\n",
       "  'The mother tongue of Douglas Adams is',\n",
       "  'The mother tongue of Paul McCartney is',\n",
       "  'Elvis Presley is a native speaker of',\n",
       "  'Barack Obama, speaker of',\n",
       "  'Douglas Adams, speaker of',\n",
       "  'Meryl Streep, a native',\n",
       "  'George Orwell spoke the language',\n",
       "  'George Washington, a native',\n",
       "  'Michael Jackson, a native'],\n",
       " 'generation_prompts': [\"Danielle Darrieux's mother tongue is\",\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  \"Danielle Darrieux's mother tongue is\",\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Where Danielle Darrieux is from, people speak the language of',\n",
       "  'Danielle Darrieux was born in',\n",
       "  'Danielle Darrieux was born in']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = datasets.Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts'],\n",
       "    num_rows: 21919\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 18:17:46 config.py:1193] Casting torch.float32 to torch.float16.\n",
      "INFO 06-17 18:17:46 config.py:1214] Downcasting torch.float32 to torch.float16.\n",
      "INFO 06-17 18:17:46 llm_engine.py:161] Initializing an LLM engine (v0.5.0) with config: model='gpt2', speculative_config=None, tokenizer='gpt2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=gpt2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 18:17:46 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 06-17 18:17:47 weight_utils.py:261] No model.safetensors.index.json found in remote.\n",
      "INFO 06-17 18:17:47 model_runner.py:159] Loading model weights took 0.2378 GB\n",
      "INFO 06-17 18:17:48 gpu_executor.py:83] # GPU blocks: 34386, # CPU blocks: 7281\n",
      "INFO 06-17 18:17:49 model_runner.py:878] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-17 18:17:49 model_runner.py:882] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-17 18:17:58 model_runner.py:954] Graph capturing finished in 9 secs.\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s, Generation Speed: 389.05 toks/s]\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(\"hello\", sampling_params=SamplingParams(max_tokens=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(batch):\n",
    "    continuations = []\n",
    "    # flatten generation prompts\n",
    "    generation_prompts = []\n",
    "    for gen_list in batch[\"generation_prompts\"]:\n",
    "        generation_prompts.extend(gen_list)\n",
    "    continuations = model.generate(\n",
    "        generation_prompts,\n",
    "        sampling_params=SamplingParams(max_tokens=50, temperature=0.7, top_k=50),\n",
    "    )\n",
    "    idx = 0\n",
    "    reshaped_continuations = []\n",
    "    for gen_list in batch[\"generation_prompts\"]:\n",
    "        chunk = [\n",
    "            prompt + out.outputs[0].text\n",
    "            for prompt, out in zip(\n",
    "                generation_prompts[idx : idx + len(gen_list)],\n",
    "                continuations[idx : idx + len(gen_list)],\n",
    "            )\n",
    "        ]\n",
    "        reshaped_continuations.append(chunk)\n",
    "        idx += len(gen_list)\n",
    "    batch[\"generation_continuations\"] = reshaped_continuations\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_data at 0x7efba1469ab0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f755a250f3de4e9fbd38b959e2728a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.47it/s, Generation Speed: 6337.01 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.74it/s, Generation Speed: 6304.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.99it/s, Generation Speed: 6322.91 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.04it/s, Generation Speed: 6326.65 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.78it/s, Generation Speed: 6301.06 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.64it/s, Generation Speed: 6349.81 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.10it/s, Generation Speed: 6344.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.15it/s, Generation Speed: 6325.85 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.79it/s, Generation Speed: 6347.29 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:16<00:00, 129.88it/s, Generation Speed: 6360.41 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.26it/s, Generation Speed: 6290.37 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.54it/s, Generation Speed: 6295.38 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.82it/s, Generation Speed: 6311.99 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.67it/s, Generation Speed: 6339.82 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:16<00:00, 130.35it/s, Generation Speed: 6383.27 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 129.68it/s, Generation Speed: 6361.96 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.47it/s, Generation Speed: 6306.20 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.87it/s, Generation Speed: 6309.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.58it/s, Generation Speed: 6306.61 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:17<00:00, 128.66it/s, Generation Speed: 6301.52 toks/s]\n",
      "Processed prompts: 100%|██████████| 10000/10000 [01:18<00:00, 128.09it/s, Generation Speed: 6286.16 toks/s]\n",
      "Processed prompts: 100%|██████████| 9190/9190 [01:11<00:00, 128.84it/s, Generation Speed: 6312.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "#processed_subset = hf_dataset.select(range(1000)).map(process_data, batched=True)\n",
    "processed_subset = hf_dataset.map(process_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['case_id', 'pararel_idx', 'requested_rewrite', 'paraphrase_prompts', 'neighborhood_prompts', 'attribute_prompts', 'generation_prompts', 'generation_continuations'],\n",
       "    num_rows: 21919\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1fcf5610d149808adb96cb979f1fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_subset.save_to_disk(\"../assets/data/processed_counterfact_full_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "editor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
